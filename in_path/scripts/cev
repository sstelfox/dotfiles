#!/usr/bin/env -S uv run -q -s

# vim: set ts=4 sw=4 noexpandtab ai :

# /// script
# requires-python = ">=3.12"
# dependencies = ["jsonschema>=4.23", "cryptography>=41.0.0"]
# ///

# todo: remove non-JSON file formats...
# todo: consider command timeout handling again, it may be worth loosing the live process logs from
#   the async process execution in favor of simplicity...
# todo: handle commands being piped into the cev process
# todo: secrets should include handler kind
# todo: secret grants should support wildcards for the matches

import argparse
import base64
import fcntl
import hashlib
import json
import os
import re
import secrets
import signal
import subprocess
import sys
import time

from collections import defaultdict
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.hkdf import HKDF
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone

from enum import Enum
from operator import itemgetter
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import jsonschema

VERSION = "0.4.0"


class LogLevel(str, Enum):
	TRACE = "trace"
	DEBUG = "debug"
	INFO = "info"
	WARN = "warn"
	ERROR = "error"
	FATAL = "fatal"
	INVALID = "invalid"

	@classmethod
	def from_string(cls, value: str) -> "LogLevel":
		try:
			return cls(value)
		except ValueError:
			return cls.INVALID

	@property
	def level_value(self) -> int:
		levels = {
			LogLevel.TRACE: 0,
			LogLevel.DEBUG: 1,
			LogLevel.INFO: 2,
			LogLevel.WARN: 3,
			LogLevel.ERROR: 4,
			LogLevel.FATAL: 5,
			LogLevel.INVALID: 6,
		}

		return levels.get(self, 6)

	def __ge__(self, other: "LogLevel") -> bool:
		return self.level_value >= other.level_value

	def __gt__(self, other: "LogLevel") -> bool:
		return self.level_value > other.level_value

	def __le__(self, other: "LogLevel") -> bool:
		return self.level_value <= other.level_value

	def __lt__(self, other: "LogLevel") -> bool:
		return self.level_value < other.level_value


class CevLogger:
	def __init__(self, log_file: Path, min_level: LogLevel = LogLevel.INFO):
		self.min_level = min_level

		self.log_file = log_file
		self.log_file.parent.mkdir(parents=True, exist_ok=True)

	def log(self, level: LogLevel, message: str, **kwargs):
		timestamp = datetime.now(timezone.utc).isoformat()

		# Prevent these from being injected, we always want to be the authority of the timestamp, the
		# others must be provided through the explicit arguments to prevent bad data from obscurring
		# messages.
		reserved_keys = {"timestamp", "level", "message"}
		safe_kwargs = {k: v for k, v in kwargs.items() if k not in reserved_keys}

		log_entry = {
			"timestamp": timestamp,
			"level": level.value,
			"message": message,
			**safe_kwargs
		}

		if level >= self.min_level:
			with open(self.log_file, "a") as f:
				f.write(json.dumps(log_entry) + "\n")

			context_str = ""
			if safe_kwargs:
				context_parts = [f"{k}={repr(v)}" for k, v in safe_kwargs.items()]
				context_str = f" ({', '.join(context_parts)})"

			# Create a readable message format: [TIMESTAMP] LEVEL: MESSAGE (key1=value1, key2=value2, ...)
			readable_output = f"[{timestamp}] {level.value.upper()}: {message}{context_str}"
			sys.stderr.write(f"{readable_output}\n")

	def trace(self, message: str, **kwargs):
		self.log(LogLevel.TRACE, message, **kwargs)

	def debug(self, message: str, **kwargs):
		self.log(LogLevel.DEBUG, message, **kwargs)

	def info(self, message: str, **kwargs):
		self.log(LogLevel.INFO, message, **kwargs)

	def warn(self, message: str, **kwargs):
		self.log(LogLevel.WARN, message, **kwargs)

	def error(self, message: str, **kwargs):
		self.log(LogLevel.ERROR, message, **kwargs)

	def fatal(self, message: str, **kwargs):
		self.log(LogLevel.FATAL, message, **kwargs)
		sys.exit(127)


class CevSecrets:
	def _decrypt(self, data: bytes, key: bytes) -> bytes:
		if len(data) < 12:
			raise ValueError("archived secret data too short to contain nonce")

		nonce = data[:12]
		ciphertext = data[12:]
		aesgcm = AESGCM(key)

		return aesgcm.decrypt(nonce, ciphertext, None)

	def _derive_key(self, key_material: str) -> bytes:
		digest = hashes.Hash(hashes.SHA256())
		digest.update(key_material.encode())

		return digest.finalize()

	def _encrypt(self, data: bytes, key: bytes) -> bytes:
		nonce = secrets.token_bytes(12)
		aesgcm = AESGCM(key)
		ciphertext = aesgcm.encrypt(nonce, data, None)

		return nonce + ciphertext

	def _get_encryption_key(self) -> Optional[bytes]:
		key_material = os.environ.get("CEV_SECRET_KEY")
		if key_material:
			return self._derive_key(key_material)

		return None

	def __init__(self, cfg_dir: Path, data_dir: Path):
		self._acl = {}
		self._loaded = False
		self._secrets = {}

		self.secrets_file = data_dir / "secrets.arxiv"

	def delete_secret(self, name: str) -> bool:
		if not self._loaded and not self.load():
			return False

		name = name.replace("-", "_").lower()
		if name in self._secrets:
			del self._secrets[name]

			for trigger_key in self._acl:
				if name in self._acl[trigger_key]:
					self._acl[trigger_key].remove(name)

			return self.save()

		return True

	def get_trigger_env(self, event_stream: str, kind: str, trigger_name: str) -> Dict[str, str]:
		if not self._loaded and not self.load():
			return {}

		trigger_key = f"{event_stream}:{kind}:{trigger_name}"
		result = {}

		if trigger_key in self._acl:
			for secret_name in self._acl[trigger_key]:
				if secret_name in self._secrets:
					CevContext.debug("exposing secret to trigger", secret_name=secret_name)
					env_var_name = f"CEV_SECRET_{secret_name.upper()}"
					result[env_var_name] = self._secrets[secret_name]

		return result

	def grant_access(self, event_stream: str, kind: str, trigger_name: str, secret_name: str) -> bool:
		if not self._loaded and not self.load():
			return False

		secret_name = secret_name.replace("-", "_").lower()
		if secret_name not in self._secrets:
			CevContext.error("cannot grant access to nonexistent secret", secret_name=secret_name)
			return False

		trigger_key = f"{event_stream}:{kind}:{trigger_name}"
		if trigger_key not in self._acl:
			self._acl[trigger_key] = []

		if secret_name not in self._acl[trigger_key]:
			self._acl[trigger_key].append(secret_name)

		return self.save()

	def list_secrets(self) -> List[Tuple[str, List[str]]]:
		if not self._loaded and not self.load():
			return []

		result = []
		for secret_name in sorted(self._secrets.keys()):
			triggers = []

			for trigger_key, secret_list in self._acl.items():
				if secret_name in secret_list:
					triggers.append(trigger_key)

			result.append((secret_name, sorted(triggers)))

		return result

	def load(self) -> bool:
		key = self._get_encryption_key()
		if not key:
			CevContext.warn("no secrets key provided, secrets not available")
			return False

		if not self.secrets_file.exists():
			CevContext.debug("no secrets archive exists")

			self._acl = {}
			self._loaded = True
			self._secrets = {}

			return True

		try:
			encoded_data = self.secrets_file.read_bytes()
			encrypted_data = base64.b64decode(encoded_data)
			decrypted_data = self._decrypt(encrypted_data, key)
			data = json.loads(decrypted_data.decode())

			self._acl = data.get("acl", {})
			self._secrets = data.get("secrets", {})

			self._loaded = True

			CevContext.debug("secrets loaded successfully", secret_count=len(self._secrets), acl_entry_count=len(self._acl))

			return True
		except Exception as e:
			CevContext.error("failed to load existing secrets", error=str(e))
			return False

	def revoke_access(self, event_stream: str, kind: str, trigger_name: str, secret_name: str) -> bool:
		if not self._loaded and not self.load():
			return False

		secret_name = secret_name.replace("-", "_").lower()
		trigger_key = f"{event_stream}:{kind}:{trigger_name}"
		if trigger_key in self._acl and secret_name in self._acl[trigger_key]:
			self._acl[trigger_key].remove(secret_name)
			return self.save()

		return True

	def save(self) -> bool:
		if not self._loaded:
			CevContext.error("cannot save secrets that haven't been loaded")
			return False

		key = self._get_encryption_key()
		if not key:
			CevContext.error("no secrets key provided, cannot save secrets")
			return False

		try:
			data = {
				"secrets": self._secrets,
				"acl": self._acl
			}

			serialized = json.dumps(data).encode()
			encrypted_data = self._encrypt(serialized, key)
			encoded_data = base64.b64encode(encrypted_data)

			self.secrets_file.parent.mkdir(parents=True, exist_ok=True)
			self.secrets_file.write_bytes(encoded_data)

			CevContext.debug("secrets saved successfully")

			return True
		except Exception as e:
			CevContext.error("failed to save secrets", error=str(e))
			return False

	def set_secret(self, name: str, value: str) -> bool:
		if not self._loaded and not self.load():
			return False

		name = name.replace("-", "_").lower()
		self._secrets[name] = value

		return self.save()


_CURRENT_CONTEXT = {}

class CevContext:
	@staticmethod
	def clear(key: str) -> Optional[Any]:
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		if key == "ctx_id":
			return None

		return _CURRENT_CONTEXT["attrs"].pop(key, None)

	@staticmethod
	def cfg_dir() -> str:
		if "cfg_dir" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return _CURRENT_CONTEXT["cfg_dir"]

	@staticmethod
	def ctx_id() -> str:
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return _CURRENT_CONTEXT["attrs"]["ctx_id"]

	@staticmethod
	def data_dir():
		if "data_dir" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return _CURRENT_CONTEXT["data_dir"]

	@staticmethod
	def init(cfg_dir: Path, data_dir: Path, min_log_level: str = "info"):
		global _CURRENT_CONTEXT
		log_file = data_dir / "system.log"

		# Generate a random ID to differentiate between different runs of the tool
		import random
		import string

		chars = string.ascii_lowercase + string.digits
		ctx_id = ''.join(random.choices(chars, k=8))

		_CURRENT_CONTEXT = {
			"logger": CevLogger(log_file, LogLevel.from_string(min_log_level)),
			"cfg_dir": cfg_dir,
			"data_dir": data_dir,
			"attrs": {
				"ctx_id": ctx_id
			}
		}

	@staticmethod
	def log(level, message, **kwargs):
		if "logger" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		# Merge context attributes with kwargs (kwargs take precedence)
		merged_kwargs = _CURRENT_CONTEXT["attrs"].copy()
		merged_kwargs.update(kwargs)

		_CURRENT_CONTEXT["logger"].log(level, message, **merged_kwargs)

	@staticmethod
	def secrets() -> "CevSecrets":
		if "cfg_dir" not in _CURRENT_CONTEXT or "data_dir" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return CevSecrets(
			_CURRENT_CONTEXT["cfg_dir"],
			_CURRENT_CONTEXT["data_dir"]
		)

	@staticmethod
	def set(key: str, value: Any):
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		if key == "ctx_id":
			return

		_CURRENT_CONTEXT["attrs"][key] = value

	@staticmethod
	def context(**kwargs):
		return _TempContextManager(kwargs)

	@staticmethod
	def trace(message, **kwargs):
		CevContext.log(LogLevel.TRACE, message, **kwargs)

	@staticmethod
	def debug(message, **kwargs):
		CevContext.log(LogLevel.DEBUG, message, **kwargs)

	@staticmethod
	def info(message, **kwargs):
		CevContext.log(LogLevel.INFO, message, **kwargs)

	@staticmethod
	def warn(message, **kwargs):
		CevContext.log(LogLevel.WARN, message, **kwargs)

	@staticmethod
	def error(message, **kwargs):
		CevContext.log(LogLevel.ERROR, message, **kwargs)

	@staticmethod
	def fatal(message, **kwargs):
		CevContext.log(LogLevel.FATAL, message, **kwargs)
		sys.exit(127)


class _TempContextManager:
	def __init__(self, kwargs):
		self.kwargs = kwargs
		self.previous = {}

	def __enter__(self):
		for key, value in self.kwargs.items():
			self.previous[key] = _CURRENT_CONTEXT["attrs"].get(key)
			CevContext.set(key, value)

		return self

	def __exit__(self, exc_type, exc_val, exc_tb):
		for key, value in self.previous.items():
			if value is None:
				CevContext.clear(key)
			else:
				CevContext.set(key, value)


class CevLock:
	def __init__(self, lock_dir: Path, lock_type: str = "state"):
		valid_lock_types = {"config", "event", "state"}
		if lock_type not in valid_lock_types:
			CevContext.fatal("unknown lock type requested", lock_type=lock_type)

		if lock_type in {"config", "event"}:
			self.wait_timeout_seconds = 5
		else:
			self.wait_timeout_seconds = 15

		self.lock_file = lock_dir / f"{lock_type}.flock"
		self.lock_type = lock_type
		self.lock_fd = None

	def __enter__(self):
		self.lock_file.parent.mkdir(parents=True, exist_ok=True)
		self.lock_fd = open(self.lock_file, "w+")
		lock_acquired = False

		CevContext.trace("attempting to retrieve lock")

		try:
			fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
			lock_acquired = True

			CevContext.set(f"{self.lock_type}_lock", True)
			CevContext.trace("lock acquired")
		except BlockingIOError:
			CevContext.trace("waiting for held lock", lock_type=self.lock_type)
			start_time = time.time()

			while time.time() - start_time < self.wait_timeout_seconds:
				try:
					fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)

					CevContext.set(f"{self.lock_type}_lock", True)
					CevContext.trace("acquired lock after wait", lock_type=self.lock_type, wait_duration=time.time() - start_time)

					lock_acquired = True
					break
				except BlockingIOError:
					time.sleep(0.1)

		if not lock_acquired:
			CevContext.fatal("could not acquire lock within timeout", lock_type=self.lock_type, timeout=self.wait_timeout_seconds, path=self.lock_file)

		# Write PID to the file after acquiring the lock, can be useful in diagnostics
		self.lock_fd.write(str(os.getpid()))
		self.lock_fd.flush()

		os.fsync(self.lock_fd.fileno())

		return self

	def __exit__(self, exc_type, exc_val, exc_tb):
		try:
			if self.lock_fd:
				fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
				self.lock_fd.close()
		finally:
			self.lock_fd = None
			CevContext.clear(f"{self.lock_type}_lock")
			CevContext.trace("lock released", **{f"{self.lock_type}_lock": False})


def create_smart_symlink(link_path: Path, target_path: Path) -> bool:
	"""
	Create a symlink that uses relative paths when the target is inside the data directory.

	Parameters:
	- link_path: Path object where the symlink will be created (must be inside data_dir)
	- target_path: Path object to the target the symlink will point to

	All paths should already be resolved.
	"""
	data_dir = CevContext.data_dir()

	# verify the link is inside the data directory
	if data_dir not in link_path.parents and link_path != data_dir:
		CevContext.error("link path must be inside data directory")
		return False

	# Determine if target is inside data directory, we'll either create a relative link or an
	# absolute link depending on what we link to.
	if data_dir in target_path.parents or target_path == data_dir:
		rel_target = target_path.relative_to(link_path.parent)
		link_path.symlink_to(rel_target)
	else:
		link_path.symlink_to(target_path)

	return True


def get_config_directory(cfg_dir: Optional[Path]) -> Path:
	if cfg_dir:
		return Path(cfg_dir).expanduser().resolve()

	env_cfg_dir = os.environ.get("CEV_CFG_DIR")
	if env_cfg_dir:
		return Path(env_cfg_dir).expanduser().resolve()

	xdg_data_home = os.environ.get("XDG_CONFIG")
	if xdg_data_home:
		return Path(xdg_data_home) / "cev"

	return Path.home() / ".config" / "cev"


def get_data_directory(data_dir: Optional[Path]) -> Path:
	if data_dir:
		return Path(data_dir).expanduser().resolve()

	env_data_dir = os.environ.get("CEV_DATA_DIR")
	if env_data_dir:
		return Path(env_data_dir).expanduser().resolve()

	xdg_data_home = os.environ.get("XDG_DATA_HOME")
	if xdg_data_home:
		return Path(xdg_data_home) / "cev"

	return Path.home() / ".local" / "share" / "cev"


def execute_commands(commands: List[Dict[str, Any]]) -> bool:
	CevContext.debug("executing event generated commands", count=len(commands))

	try:
		for index, cmd in enumerate(commands):
			command_type = cmd.pop("command")
			CevContext.debug("executing command event", command_type=command_type, index=index)

			if command_type == "add-event":
				successful = cmd_add_event(AddEventArgs(**cmd))
			elif command_type == "alias-trigger":
				successful = cmd_alias_trigger(AliasTriggerArgs(**cmd))
			elif command_type == "define-event":
				successful = cmd_define_event(DefineEventArgs(**cmd))
			elif command_type == "define-trigger":
				successful = cmd_define_trigger(DefineTriggerArgs(**cmd))
			elif command_type == "unwatch":
				successful = cmd_unwatch(UnwatchArgs(**cmd))
			elif command_type == "watch":
				successful = cmd_watch(WatchArgs(**cmd))
			else:
				CevContext.error("unknown command type", command_type=command_type, index=index)
				return False

			if not successful:
				CevContext.warn("internal command failed", command_type=command_type, cmd=cmd, index=index)
				return False

		return True
	except ValueError as err:
		CevContext.error("command failed to load into appropriate type", error=str(err))
		return False


def execute_extraction_queue() -> bool:
	CevContext.debug("executing extraction events using built-in handler")

	event_stream = "system-events/extraction"
	event_stream_dir = CevContext.data_dir() / event_stream

	successful = True

	with CevLock(event_stream_dir, "state"):
		state = read_event_stream_state(event_stream)
		if not state:
			CevContext.debug("no extraction tasks available")
			return True

		# Group tasks by watcher
		tasks_by_watcher = defaultdict(list)
		for task in state.get("tasks", []):
			tasks_by_watcher[task["watcher"]].append(task)

		for watching_stream, tasks in tasks_by_watcher.items():
			sorted_tasks = sorted(tasks, key=itemgetter("task_id"))
			CevContext.debug("processing extractions for watcher", watching_stream=watching_stream, task_count=len(sorted_tasks))

			for task in sorted_tasks:
				target_state_dir = CevContext.data_dir() / task["target"] / "state"
				new_state_path = target_state_dir / task["new_state_name"]
				extra_args = [str(new_state_path)]

				previous_state_name = task.get("previous_state_name")
				if previous_state_name:
					previous_state_path = target_state_dir / previous_state_name
					extra_args.append(str(previous_state_path))

				exit_code, _, commands = run_trigger(task["watcher"], "extractor", task["extractor"], extra_args)

				# If the extractor fails, don't process any further queued extraction events for this stream
				# until its resolved.
				if exit_code != 0:
					successful = False
					break

				if commands:
					CevContext.debug("extractor produced commands", cmd_count=len(commands))

					# Extractors make changes through data events only, other commands are restricted
					if is_valid_command_list(commands, True):
						if not execute_commands(commands):
							CevContext.error("failed to execute commands produced by extractor")
							successful = False
							break
					else:
						CevContext.error("extractor produced invalid command list")
						successful = False
						break

				cmd_add_event(AddEventArgs(
					event_stream="system-events/extraction",
					event_name="complete-task",
					payload={
						"task_id": task["task_id"],
						"ctx_id": CevContext.ctx_id(),
						"generated_events": len(commands),
					}
				))


	# Process any emitted events to ensure the resulting state file is inspectable
	if not cmd_process("system-events/extraction"):
		CevContext.error("failed to process extraction queue after executing tasks")
		return False

	return successful


def execute_scheduled_queue(extra_args: Optional[List[str]]) -> bool:
	CevContext.debug("executing scheduled events using built-in handler")

	if extra_args:
		parser = argparse.ArgumentParser(description="Common Event system - Scheduled Event Execution")
		parser.add_argument("-t", "--limit-time", type=int, help="Specify a time limit (in seconds) for processing all outstanding tasks during this run. Specifying a time limit of 25 seconds or less will prevent warning signals from triggering")

		args = parser.parse_args(extra_args)
		#if args.limit_time:
		#	CevContext.debug("setting user specified time limit for event processing", time_limit=args.limit_time)
		#	runtime_limit = args.limit_time

	event_stream = "system-events/scheduled"
	event_stream_dir = CevContext.data_dir() / event_stream

	with CevLock(event_stream_dir, "state"):
		state = read_event_stream_state(event_stream)
		if not state:
			CevContext.debug("no scheduled tasks available")
			return True

		now_dt = datetime.now(timezone.utc)
		now = now_dt.isoformat()

		due_tasks = []
		for task in state["tasks"]:
			scheduled_dt = datetime.fromisoformat(task["scheduled_for"])
			if scheduled_dt <= now_dt:
				due_tasks.append(task)

		due_tasks.sort(key=lambda x: x.get("scheduled_for"))

		if len(due_tasks) == 0:
			CevContext.debug("no tasks due for execution; scheduled task execution complete")
			return True

		CevContext.trace("scheduled task state", state=state, tasks_ready=len(due_tasks))

		start_time = time.time()
		executed_task_count = 0
		failed_streams = set()

		for task in due_tasks:
			# todo: restore
			#time_elapsed = time.time() - start_time
			#time_remaining = runtime_limit - time_elapsed

			#if time_remaining < 25:
			#	CevContext.warn("insufficient buffer time remaining, skipping remaining tasks")
			#	break

			if task["event_stream"] in failed_streams:
				CevContext.debug("skipping ready task in failed stream")
				continue

			success, _ = cmd_run(task["event_stream"], task["trigger_name"], task.get("extra_args"))
			if not success:
				failed_streams.add(task["event_stream"])

			status = "success" if success else "failure"
			cmd_add_event(AddEventArgs(
				event_stream="system-events/scheduled",
				event_name="complete-task",
				payload={ "task_id": task["task_id"], "ctx_id": CevContext.ctx_id(), "status": status }
			))

			executed_task_count += 1
			CevContext.trace("registered task completion", task_id=task["task_id"], event_stream=task["event_stream"], trigger_name=task["trigger_name"])

	# Process any emitted events to ensure the resulting state file is inspectable
	if not cmd_process("system-events/scheduled"):
		CevContext.error("failed to process scheduled event queue after executing outstanding tasks")
		return False

	remaining = len(due_tasks) - executed_task_count
	CevContext.debug("scheduled task execution complete", executed=executed_task_count, remaining=remaining)
	return True


def process_stderr_buffer(buffer: str, output_commands: List[Dict[str, Any]]) -> str:
	if '\n' in buffer:
		lines = buffer.split('\n')
		remaining = lines.pop()

		for line in lines:
			command = filter_trigger_command(line)
			if command:
				output_commands.append(command)

		return remaining

	return buffer


def filter_trigger_command(line: str) -> Optional[Dict[str, Any]]:
	if not line.strip():
		return None

	try:
		data = json.loads(line)
	except json.JSONDecodeError:
		# It's not JSON so its probably a raw runtime error. This is truly exceptional output we can't
		# handle and probably the result of a problem in the user's script. These are frequently
		# multi-line which will come in all at once. We bypass the standard logging message and dump it
		# straight to stderr so a user running it interactively can see the messages without them being
		# obscurred by the json formatting.
		#
		# In the future I may want to capture all of these in a diagnostic log tied to the context ID but
		# this works surprisingly well.
		sys.stderr.write(f"{line}\n")
		return None

	if not isinstance(data, dict):
		CevContext.warn("non-command json output on stderr", stderr=line)
		return None

	if "command" in data:
		return data

	# Try and treat the line as a log message which only requires a message key
	message = data.pop("message", None)
	if message is None:
		CevContext.warn("trigger produced JSON output but was neither command nor log", content=data)
		return None

	log_level = LogLevel.from_string(data.pop("level", "info"))
	CevContext.log(log_level, message, **data)


def generate_stub_from_schema(schema: Dict[str, Any]) -> Optional[Any]:
	schema_type = schema.get("type")
	if not schema_type:
		return {}

	if schema_type == "object":
		result = {}

		for prop_name, prop_schema in schema.get("properties", {}).items():
			result[prop_name] = generate_stub_from_schema(prop_schema)

		return result
	elif schema_type == "array":
		if "items" in schema:
			return [generate_stub_from_schema(schema["items"])]

		return []
	elif schema_type == "string":
		if schema.get("format") == "date-time":
			return datetime.now(timezone.utc).isoformat()

		return ""
	elif schema_type == "boolean":
		return False
	elif schema_type == "number":
		return 0.0
	elif schema_type == "integer":
		return 0
	elif schema_type == "null":
		return None

	CevContext.warn("unrecognized schema type, leaving as null", schema_type=schema_type)
	return None


def get_active_schema_versions(event_stream: str) -> Dict[str, str]:
	active_versions = {}

	active_version_path = CevContext.data_dir() / event_stream / "schemas" / "active_versions"
	if not active_version_path.exists():
		CevContext.trace("no active schema versions for event stream", event_stream=event_stream)
		return active_versions

	with open(active_version_path, "r") as f:
		for line in f:
			line = line.strip()
			if line:
				active_versions[line.split('.')[0]] = line

	return active_versions


def get_schema(event_stream: str, trigger_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking up schema", event_stream=event_stream, trigger_name=trigger_name)

	if event_stream.startswith("system-events"):
		if not trigger_name:
			trigger_name = "state"

		event_stream = event_stream.replace("/", "-")

		return get_embedded_schema(f"{event_stream}-{trigger_name}-schema")
	else:
		if not trigger_name:
			return None

		return get_user_schema(event_stream, trigger_name)


def get_embedded_schema(key: str) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking for embedded schema", key=key)

	# We know this script is self-executing as we've explicitly restricted it to only allow
	# self-execution
	script_path = Path(sys.argv[0])
	with open(script_path, "r") as f:
		script_content = f.read()

	if "__EMBEDDED_SCHEMAS__" not in script_content:
		CevContext.fatal("missing schema section in script")

	schema_section = script_content.split("__EMBEDDED_SCHEMAS__", 1)[1]

	current_key = None
	content_lines = []
	schema_text = None

	# Parse the data section to find the requested key
	for line in schema_section.split("\n"):
		if line.startswith("# [") and line.endswith("]"):
			# We found the next defined schema
			if current_key == key and content_lines:
				schema_text = "\n".join(content_lines)

			# Collect the lines until we reach the next section so we have all its contents
			current_key = line[3:-1] # Remove "# [" and "]"
			content_lines = []
		elif current_key == key:
			content_lines.append(line)

	# The last schema was the one we were looking for
	if current_key == key and content_lines:
		schema_text = "\n".join(content_lines)

	if not schema_text:
		return None

	try:
		return json.loads(schema_text)
	except json.JSONDecodeError as e:
		CevContext.fatal("failed to decode built-in schema", error=str(e), schema_key=key)


def get_latest_state_file(event_stream: str) -> Optional[Path]:
	state_dir = CevContext.data_dir() / event_stream / "state"
	if not state_dir.exists():
		return None

	# Might be more efficient to read the processed index and find the matching state... but that
	# won't work for events that don't modify state...
	snapshots = sorted(state_dir.glob("*.state.json"), key=lambda x: str(x))
	if not snapshots:
		return None

	return snapshots[-1]


def get_stream_index(event_stream: str, index_kind: str) -> Optional[int]:
	index_file = CevContext.data_dir() / event_stream / f"{index_kind}.idx"
	if not index_file.exists():
		return None

	return int(index_file.read_text().strip(), 16)


def get_user_schema(event_stream: str, event_name: str) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking for user schema")

	active_versions = get_active_schema_versions(event_stream)

	schema_version = active_versions.get(event_name)
	if not schema_version:
		CevContext.error("event name doesn't have an active schema")
		return None

	schema_dir = CevContext.data_dir() / event_stream / "schemas"
	schema_path = schema_dir / f"{schema_version}.json"

	return read_json_file(schema_path)


def get_watchers(event_stream: str) -> Dict[str, List[str]]:
	watchers_file = CevContext.data_dir() / event_stream / "watchers"
	watchers = {}

	if not watchers_file.exists():
		return watchers

	with open(watchers_file, "r") as f:
		for line in f:
			entry = line.strip()

			if entry and ":" in entry:
				watcher, trigger = entry.split(":", 1)

				if not watchers.get(watcher):
					watchers[watcher] = []

				watchers[watcher].append(trigger)

	return watchers


def meta_from_event_file(file: Path) -> Tuple[Optional[int], Optional[str], Optional[str]]:
	match = re.match(r'^([0-9a-f]{8})-(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[\-+]\d{2}:\d{2})-([a-z0-9\-]{3,64})\.evt$', file.name)

	if match:
		event_id_hex, recorded_at, trigger_name = match.groups()
		event_id = int(event_id_hex, 16)
		return event_id, recorded_at, trigger_name
	else:
		return None, None, None


@dataclass
class StateMetadata:
	event_stream: str
	stream_idx: str
	timestamp: str
	schema_versions: Dict[str, str]
	watched_streams: List[str]

@dataclass
class Envelope:
	metadata: StateMetadata
	state: Dict[str, Any]

def persist_state(event_stream: str, stream_idx: int, state_payload: Dict[str, Any]):
	current_state_path = get_latest_state_file(event_stream)
	current_state_name = current_state_path.name if current_state_path else None

	watchers = get_watchers(event_stream)
	metadata = StateMetadata(
			event_stream=event_stream,
			stream_idx=f"{stream_idx:08x}",
			timestamp=datetime.now(timezone.utc).isoformat(),
			schema_versions=get_active_schema_versions(event_stream),
			watched_streams=watchers
	)

	envelope = asdict(Envelope(metadata=metadata, state=state_payload))
	CevContext.trace("persisting new state", envelope=envelope)

	state_dir = CevContext.data_dir() / event_stream / "state"
	state_dir.mkdir(parents=True, exist_ok=True)

	state_path = state_dir / f"{stream_idx:08x}-{metadata.timestamp}.state.json"
	with open(state_path, "w") as f:
		json.dump(envelope, f, indent=2)

	# If there were interested watchers, we need to let them know about the new version
	if watchers:
		for watcher, extractors in watchers.items():
			for extr in extractors:
				schedule_extraction(watcher, event_stream, extr, state_path.name, current_state_name)

	CevContext.debug("persisted event stream state")


def read_event_stream_state(event_stream: str) -> Optional[Dict[str, Any]]:
	latest_snapshot_path = get_latest_state_file(event_stream)
	if not latest_snapshot_path:
		CevContext.debug("no state available for event stream")
		return None

	envelope = read_json_file(latest_snapshot_path)
	if not envelope:
		CevContext.error("unable to load state file")
		return None

	return envelope.get("state")


def read_json_file(file_path: Path) -> Optional[Dict[str, Any]]:
	try:
		with open(file_path, "r") as f:
			return json.load(f)
	except (json.JSONDecodeError, FileNotFoundError) as e:
		CevContext.fatal("failed to load data file", error=str(e))


def set_watchers(event_stream: str, watchers: Dict[str, List[str]]):
	event_stream_dir = CevContext.data_dir() / event_stream
	with CevLock(event_stream_dir, "state"):

		watchers_file = event_stream_dir / "watchers"
		with open(watchers_file, "w") as f:
			for watching_stream, extractors in watchers.items():
				for extractor in extractors:
					f.write(f"{watching_stream}:{extractor}\n")


def subprocess_executor(cmd: List[str], env_vars: Dict[str, str]) -> Tuple[int, Optional[Dict[str, Any]], List[Dict[str, Any]]]:
	reportable_env_vars = env_vars.copy()
	for env_name in reportable_env_vars.keys():
		if env_name.startswith("CEV_SECRET_"):
			reportable_env_vars[env_name] = "***"

	CevContext.debug("executing subprocess", cmd=str(cmd), env_vars=reportable_env_vars)

	import select
	import fcntl
	import errno

	target_time_limit = 30
	kill_grace_period = 5
	warning_time_limit = target_time_limit - kill_grace_period
	warning_sent = False

	stdout_buffer = ""
	stderr_buffer = ""

	output_commands = []
	start_time = time.time()

	try:
		process = subprocess.Popen(cmd, env=env_vars, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

		# We use non-blocking mode to process log entries live
		for pipe in [process.stdout, process.stderr]:
			flags = fcntl.fcntl(pipe.fileno(), fcntl.F_GETFL)
			fcntl.fcntl(pipe.fileno(), fcntl.F_SETFL, flags | os.O_NONBLOCK)

		while process.poll() is None:
			current_time = time.time()
			elapsed_time = current_time - start_time

			if elapsed_time > warning_time_limit and not warning_sent:
				# Send SIGHUP first as a warning
				CevContext.warn("trigger reached warning limit timeout, sending warning SIGHUP signal")
				process.send_signal(signal.SIGHUP)
				warning_sent = True

			if elapsed_time > target_time_limit:
				CevContext.warn("trigger still running after warning signal and grace period elapsed, killing process")
				process.kill()
				break

			try:
				ready_pipes, _, _ = select.select([process.stdout, process.stderr], [], [], 0.1)
			except select.error:
				# Handle possible interruption in select
				continue

			for pipe in ready_pipes:
				try:
					if pipe == process.stdout:
						chunk = process.stdout.read()
						if chunk:
							stdout_buffer += chunk
					elif pipe == process.stderr:
						chunk = process.stderr.read()
						if chunk:
							stderr_buffer += chunk

				except (IOError, OSError) as e:
					if e.errno != errno.EAGAIN:	# Ignore would-block errors
						raise

		try:
			final_stdout = process.stdout.read()
			if final_stdout:
				stdout_buffer += final_stdout

			final_stderr = process.stderr.read()
			if final_stderr:
				stderr_buffer += final_stderr
				stderr_buffer = process_stderr_buffer(stderr_buffer, output_commands)
		except (IOError, OSError) as e:
			if e.errno != errno.EAGAIN:
				raise

		# Process any remaining stderr content after the process completes
		stderr_buffer = process_stderr_buffer(stderr_buffer, output_commands).strip()
		if stderr_buffer:
			command = filter_trigger_command(stderr_buffer)
			if command:
				output_commands.append(command)

		new_state = None
		stdout_buffer = stdout_buffer.strip()

		if len(stdout_buffer) > 0:
			try:
				new_state = json.loads(stdout_buffer)
				if not isinstance(new_state, dict):
					CevContext.warn("trigger produced non-dict JSON on stdout")
					new_state = None
			except json.JSONDecodeError:
				CevContext.warn("trigger produced invalid output on stdout")
				new_state = None

		exit_status = process.returncode if process.returncode is not None else 200

		return exit_status, new_state, output_commands
	except subprocess.TimeoutExpired:
		CevContext.error("trigger timed out")
		process.kill()
		return 201, None, []
	except Exception as e:
		CevContext.error("unexpected exception running trigger", error=str(e))
		return 237, None, []


def run_extraction_event_handler(state: Dict[str, Any], event_name: str, stream_index: int, event_path: Path) -> Tuple[bool, Optional[Dict[str, Any]]]:
	task_id = f"{stream_index:08x}"

	event = read_json_file(event_path)
	if not event:
		CevContext.error("unable to load persisted event")
		return False, None

	if event_name == "extract-task":
		CevContext.debug("planning state extraction", task_id=task_id, watching_stream=event["watcher"], target_stream=event["target"])

		extract_tasks = state.get("tasks", [])
		extract_tasks.append(event | {"task_id": task_id})

		state_update = { "tasks": extract_tasks }
		return True, state_update
	elif event_name == "complete-task":
		CevContext.debug("completing task", task_id=event["task_id"])

		state_update = {}

		remaining_tasks = [task for task in state["tasks"] if task["task_id"] != event["task_id"]]
		if remaining_tasks:
			state_update["tasks"] = remaining_tasks

		return True, state_update
	else:
		CevContext.error("unknown trigger for stream")
		return False, None


def run_scheduled_event_handler(state: Dict[str, Any], event_name: str, stream_index: int, event_path: Path) -> Tuple[bool, Optional[Dict[str, Any]]]:
	task_id = f"{stream_index:08x}"

	event = read_json_file(event_path)
	if not event:
		CevContext.error("unable to load persisted event")
		return False, None

	if event_name == "schedule-task":
		CevContext.debug("scheduling task", task_id=task_id, scheduled_for=event["scheduled_for"])

		tasks = state.get("tasks", [])
		tasks.append(event | { "task_id": task_id })

		state_update = { "tasks": tasks }
		return True, state_update
	elif event_name == "complete-task":
		CevContext.debug("completing task", task_id=event["task_id"], status=event["status"])

		state_update = {}

		remaining_tasks = [task for task in state["tasks"] if task["task_id"] != event["task_id"]]
		if remaining_tasks:
			state_update["tasks"] = remaining_tasks

		return True, state_update
	else:
		CevContext.error("unknown trigger for stream")
		return False, None


def run_trigger(event_stream: str, kind: str, trigger_name: str, extra_args: Optional[List[str]] = None) -> Tuple[int, Optional[Dict[str, Any]], List[Dict[str, Any]]]:
	if not is_stream_active(event_stream):
		CevContext.fatal("event stream must exist and be active to call trigger", event_stream=event_stream)

	if not is_valid_event_name(trigger_name):
		CevContext.fatal("invalid trigger name", trigger_name=trigger_name)

	CevContext.debug("running trigger", event_stream=event_stream, kind=kind, trigger_name=trigger_name, extra_args=extra_args)

	if kind == "task" and event_stream == "system-events/extraction":
		if trigger_name == "execute-queue":
			status = execute_extraction_queue()
			exit_code = 0 if status else 1
			return exit_code, None, []
		else:
			CevContext.fatal("unknown scheduled queue trigger name")

	if kind == "task" and event_stream == "system-events/scheduled":
		if trigger_name == "execute-queue":
			status = execute_scheduled_queue(extra_args)
			exit_code = 0 if status else 1
			return exit_code, None, []
		else:
			CevContext.fatal("unknown scheduled queue trigger name")

	data_dir = CevContext.data_dir()
	event_stream_dir = data_dir / event_stream

	trigger_path = event_stream_dir / f"{kind}s" / trigger_name
	if not trigger_path.exists():
		CevContext.fatal("specified trigger task does not exist with the specified type", kind=kind)

	trig_env = {}

	# Pull in a selective set of environment variables from the OS, we'll include some that are
	# relevant to cev if they're not security sensitive.
	for var_name in {"CEV_VERBOSE", "HOME", "LC_ALL", "PATH", "SHELL", "USER"}:
		var_value = os.environ.get(var_name)
		if var_value:
			trig_env[var_name] = var_value

	env_vars = {
		"CEV_BIN": str(Path(sys.argv[0]).resolve()),
		"CEV_DATA_DIR": str(data_dir),
		"CEV_EVENT_STREAM": event_stream,
		"CEV_TRIGGER_NAME": trigger_name,
		"CEV_TRIGGER_KIND": kind
	}

	latest_snapshot = get_latest_state_file(event_stream)
	if latest_snapshot:
		env_vars["CEV_LATEST_STATE_SNAPSHOT"] = str(latest_snapshot)

	trig_env.update(env_vars)

	# Before launching the process, add secret environment variables. These override anything already
	# present but are in a reserved namespace of CEV_SECRET_* that shouldn't impact others.
	secrets = CevContext.secrets()
	secret_env = secrets.get_trigger_env(event_stream, kind, trigger_name)
	trig_env.update(secret_env)

	cmd = [str(trigger_path)]
	if extra_args:
		cmd.extend(extra_args)

	return subprocess_executor(cmd, trig_env)



def schedule_extraction(watcher_stream: str, target_stream: str, extractor: str, new_state_name: str, previous_state_name: Optional[str] = None) -> bool:
	CevContext.debug("scheduling extraction for watching stream", watcher_stream=watcher_stream, extractor=extractor)

	payload = {
		"watcher": watcher_stream,
		"target": target_stream,
		"extractor": extractor,
		"new_state_name": new_state_name,
	}

	if previous_state_name:
		payload["previous_state_name"] = previous_state_name

	return cmd_add_event(AddEventArgs(
		event_stream="system-events/extraction",
		event_name="extract-task",
		payload=payload
	))


def set_stream_index(event_stream: str, index_kind: str, stream_index: int):
	CevContext.debug("updating stream index", event_stream=event_stream, index_kind=index_kind, stream_index=stream_index)
	index_file = CevContext.data_dir() / event_stream / f"{index_kind}.idx"
	index_file.write_text(f"{stream_index:08x}")


def validate_against_schema(schema: Dict[str, Any], data: Dict[str, Any]) -> bool:
	try:
		jsonschema.validate(data, schema)
		CevContext.debug("schema passed validation")
		return True
	except jsonschema.exceptions.SchemaError as e:
		CevContext.error("provided schema wasn't valid", error=str(e))
		CevContext.trace("invalid schema", schema=schema)
		return False
	except jsonschema.exceptions.ValidationError as e:
		CevContext.error("data failed to conform to provided schema", error=str(e))
		CevContext.trace("failed data and schema", schema=schema, data=data)
		return False


def validate_command(command: Dict[str, Any], restricted: bool) -> bool:
	command_identity = command.get("command")
	if not command_identity:
		CevContext.error("command identity missing")
		return False

	with CevContext.context(command=True, command_identity=command_identity, restricted=restricted):
		schema = get_embedded_schema(f"cmd-{command_identity}-command-schema")
		if not schema:
			CevContext.error("provided command not recognized")
			return False

		if restricted and command_identity != "add-event":
			CevContext.error("mutating command found in restricted command environment")
			return False

		try:
			jsonschema.validate(command, schema)
		except jsonschema.exceptions.ValidationError as e:
			CevContext.error("provided command did not validate against expected schema", error=str(e))
			return False

	return True


def is_valid_command_list(commands: List[Dict[str, Any]], restricted: bool = True) -> bool:
	for cmd in commands:
		if not validate_command(cmd, restricted):
			return False

	return True


def is_valid_event_name(event_name: str) -> bool:
	if not event_name:
		return False

	pattern = r'^([a-z0-9-]{3,64})$'
	return bool(re.match(pattern, event_name))


def is_valid_stream_name(event_stream: str) -> bool:
	if not event_stream:
		return False

	pattern = r"^[a-z0-9-]{3,32}(/[a-z0-9-]{3,32})*$"
	return bool(re.match(pattern, event_stream))


def validate_json_schema(schema: Dict[str, Any]) -> bool:
	try:
		jsonschema.Draft7Validator.check_schema(schema)
		return True
	except jsonschema.exceptions.SchemaError as e:
		return False


def is_stream_active(event_stream: str) -> bool:
	if not is_valid_stream_name(event_stream):
		CevContext.error("invalid event stream name")
		return False

	# Handle the special system event streams
	if event_stream in {"system-events/scheduled", "system-events/extraction"}:
		return True

	event_stream_dir = CevContext.data_dir() / event_stream
	if not event_stream_dir.exists():
		CevContext.error("event stream does not exist")
		return False

	return True


@dataclass
class AddEventArgs:
	event_stream: str
	event_name: str
	payload: Dict[str, Any]

def cmd_add_event(args: AddEventArgs) -> bool:
	if not is_stream_active(args.event_stream):
		CevContext.error("event stream must exist and be active to add an event", event_stream=args.event_stream)
		return False

	if not is_valid_event_name(args.event_name):
		CevContext.error("invalid event name", event_stream=args.event_stream, event_name=args.event_name)
		return False

	with CevContext.context(event_stream=args.event_stream, event_name=args.event_name):
		# Recording any event, event the system ones require a lock in the appropriate
		# directory
		event_stream_dir = CevContext.data_dir() / args.event_stream
		with CevLock(event_stream_dir, "event"):
			schema = get_schema(args.event_stream, trigger_name=args.event_name)
			if schema is None:
				CevContext.error("unknown event can't be added")
				return False

			if not validate_against_schema(schema, args.payload):
				CevContext.error("event failed to validate against schema", schema=schema, event=args.payload)
				return False

			current_index = get_stream_index(args.event_stream, "current")
			if current_index is None:
				new_index = 0
			else:
				new_index = current_index + 1

			recorded_at = datetime.now(timezone.utc).isoformat()

			events_dir = event_stream_dir / "events"
			events_dir.mkdir(parents=True, exist_ok=True)
			event_path = events_dir / f"{new_index:08x}-{recorded_at}-{args.event_name}.evt"

			with open(event_path, "w") as f:
				json.dump(args.payload, f, indent=2)

			set_stream_index(args.event_stream, "current", new_index)

		CevContext.debug("added event", stream_index=new_index)
		CevContext.trace("raw event", event=args.payload)

	return True


@dataclass
class AliasTriggerArgs:
	source_stream: str
	source_name: str
	source_kind: str = "handler"

	dest_stream: Optional[str] = None
	dest_name: Optional[str] = None
	dest_kind: Optional[str] = None

def cmd_alias_trigger(args: AliasTriggerArgs) -> bool:
	if not is_valid_stream_name(args.source_stream):
		CevContext.error("invalid source event stream name", event_stream=args.source_stream)
		return False

	if not is_valid_event_name(args.source_name):
		CevContext.error("invalid source trigger name", trigger_name=args.source_name)
		return False

	if args.source_kind not in {'handler', 'extractor', 'task'}:
		CevContext.error("undefined source trigger kind", trigger_kind=args.source_kind)
		return False

	# Default to the source values
	dest_stream = args.dest_stream if args.dest_stream else args.source_stream
	dest_name = args.dest_name if args.dest_name else args.source_name
	dest_kind = args.dest_kind if args.dest_kind else args.source_kind

	# At least one must differ or we're just copying over the source
	if dest_stream == args.source_stream and dest_name == args.source_name and dest_kind == args.source_kind:
		CevContext.error("at least one different destination parameter must be provided")
		return False

	if not is_valid_stream_name(dest_stream):
		CevContext.error("invalid destination event stream name", event_stream=dest_stream)
		return False

	if not is_valid_event_name(dest_name):
		CevContext.error("invalid destination trigger name", trigger_name=dest_name)
		return False

	if dest_kind not in {'handler', 'extractor', 'task'}:
		CevContext.error("undefined destination trigger kind", trigger_kind=dest_kind)
		return False

	with CevContext.context(
		source_stream=args.source_stream,
		source_name=args.source_name,
		source_kind=args.source_kind,
		dest_stream=dest_stream,
		dest_name=dest_name,
		dest_kind=dest_kind
	):
		data_dir = CevContext.data_dir()
		source_trigger_path = data_dir / args.source_stream / f"{args.source_kind}s" / args.source_name
		if not source_trigger_path.exists():
			CevContext.error("source trigger does not exist")
			return False

		with CevLock(data_dir, "config"):
			dest_stream_dir = data_dir / dest_stream

			with CevLock(dest_stream_dir, "event"):
				dest_trigger_dir = dest_stream_dir / f"{dest_kind}s"
				dest_trigger_dir.mkdir(parents=True, exist_ok=True)

				dest_trigger_path = dest_trigger_dir / dest_name
				if dest_trigger_path.exists():
					CevContext.error("destination trigger already exists", dest_trigger_path=str(dest_trigger_path))
					return False

				if create_smart_symlink(dest_trigger_path, source_trigger_path):
					CevContext.debug("successfully aliased trigger")
					return True
				else:
					CevContext.error("failed to create symlink for trigger")
					return False


def cmd_create_event(event_stream: str, event_name: str, output_path: Optional[Path]) -> bool:
	with CevContext.context(event_stream=event_stream, event_name=event_name):
		if not is_valid_stream_name(event_stream):
			CevContext.error("invalid event stream name")
			return False

		if not is_valid_event_name(event_name):
			CevContext.error("invalid event name")
			return False

		requested_event_schema = get_schema(event_stream, trigger_name=event_name)
		if not requested_event_schema:
			CevContext.error("unable to locate reference schema for provided event")
			return False

		stub_event = generate_stub_from_schema(requested_event_schema)

		if output_path:
			full_output_path = Path(output_path).expanduser().resolve()
			if full_output_path.exists():
				CevContext.error("provided path already exists, cowardly refusing to overwrite file")
				return False

			try:
				with open(full_output_path, "w") as f:
					json.dump(stub_event, f, indent=2)
			except Exception as e:
				CevContext.error("failed to write out event stub", error=str(e))
				return False
		else:
			print(json.dumps(stub_event, indent=2))

	CevContext.debug("created event stub based off existing schema")
	return True


@dataclass
class DefineEventArgs:
	event_stream: str
	event_name: str
	schema: Dict[str, Any]

def cmd_define_event(args: DefineEventArgs) -> bool:
	if not is_valid_stream_name(args.event_stream):
		CevContext.error("invalid event stream name", event_stream=args.event_stream)
		return False

	if not is_valid_event_name(args.event_name):
		CevContext.error("invalid event name", event_name=args.event_name)
		return False

	with CevContext.context(event_stream=args.event_stream, event_name=args.event_name):
		if not validate_json_schema(args.schema):
			CevContext.error("schema for event definition isn't valid")
			return False

		data_dir = CevContext.data_dir()
		with CevLock(data_dir, "config"):

			event_stream_dir = data_dir / args.event_stream
			with CevLock(event_stream_dir, "event"):
				# Read existing active versions we already have them
				active_versions = get_active_schema_versions(args.event_stream)

				current_date_prefix = datetime.now().strftime('%Y%m%d')
				schema_version = f"{current_date_prefix}00"

				schema_dir = event_stream_dir / "schemas"
				schema_dir.mkdir(parents=True, exist_ok=True)

				# Figure out what our new schema version is for this event name
				if args.event_name in active_versions:
					current_version = active_versions[args.event_name]
					version_parts = current_version.split('.')[-1]

					if version_parts.startswith(current_date_prefix):
						version_number = int(version_parts[8:]) + 1
						schema_version = f"{current_date_prefix}{version_number:02d}"

					existing_schema_path = schema_dir / f"{current_version}.json"
					existing_schema = read_json_file(existing_schema_path)
					if not existing_schema:
						CevContext.error("failed to read existing schema")
						return False

					if existing_schema == args.schema:
						CevContext.debug("schema unchanged, no update needed")
						return True

				schema_path = schema_dir / f"{args.event_name}.{schema_version}.json"
				with open(schema_path, "w") as f:
					json.dump(args.schema, f, indent=2)

				active_versions[args.event_name] = f"{args.event_name}.{schema_version}"

				active_version_path = event_stream_dir / "schemas/active_versions"
				with open(active_version_path, "w") as f:
					for version in sorted(active_versions.values()):
						f.write(f"{version}\n")

		CevContext.debug("defined event schema", schema_version=schema_version)
		return True


@dataclass
class DefineTriggerArgs:
	event_stream: str
	trigger_name: str
	trigger_kind: str
	trigger_path: str
	copy_trigger: bool

def cmd_define_trigger(args: DefineTriggerArgs):
	if not is_valid_stream_name(args.event_stream):
		CevContext.error("invalid event stream name", event_stream=args.event_stream)
		return

	if not is_valid_event_name(args.trigger_name):
		CevContext.error("invalid trigger name", trigger_name=args.trigger_name)
		return

	with CevContext.context(event_stream=args.event_stream, trigger_kind=args.trigger_kind, trigger_name=args.trigger_name):
		if not os.path.isfile(args.trigger_path):
			CevContext.fatal("trigger file does not exist", trigger_path=str(args.trigger_path))

		if not os.access(args.trigger_path, os.X_OK):
			CevContext.fatal("trigger file is not executable", trigger_path=str(args.trigger_path))

		data_dir = CevContext.data_dir()
		with CevLock(data_dir, "config"):
			event_stream_dir = data_dir / args.event_stream
			trigger_dir = event_stream_dir / f"{args.trigger_kind}s"

			if args.trigger_kind not in {'handler', 'extractor', 'task'}:
				CevContext.fatal("undefined trigger kind", trigger_kind=args.trigger_kind)

			with CevLock(event_stream_dir, "event"):
				trigger_dir.mkdir(parents=True, exist_ok=True)
				new_trigger_path = trigger_dir / args.trigger_name

				if args.copy_trigger:
					with open(args.trigger_path, "rb") as src, open(new_trigger_path, "wb") as trig:
						trig.write(src.read())

					new_trigger_path.chmod(0o755)
				else:
					create_smart_symlink(new_trigger_path, args.trigger_path)

		CevContext.debug("defined trigger")


def cmd_info():
	data_dir = CevContext.data_dir()
	CevContext.info("system info", data_directory=str(data_dir), version=VERSION)


def cmd_is_clean(event_stream: str) -> bool:
	with CevContext.context(event_stream=event_stream):
		if not is_stream_active(event_stream):
			CevContext.fatal("event stream must exist and be active to check dirty status")

		data_dir = CevContext.data_dir()
		event_stream_dir = data_dir / event_stream

		current_index = get_stream_index(event_stream, "current")
		if current_index is None:
			CevContext.debug("stream is clean (no events)", status="clean")
			return True

		processed_index = get_stream_index(event_stream, "processed")
		if processed_index is None:
			CevContext.debug("stream is dirty (no processed events)", status="dirty")
			return False

		with CevContext.context(current_index=current_index, processed_index=processed_index):
			if current_index > processed_index:
				CevContext.debug("stream is dirty (unprocessed events)", status="dirty")
				return False

			CevContext.debug("stream is clean", status="clean")
			return True


def cmd_is_newer(event_stream: str, stream_index: int) -> bool:
	with CevContext.context(event_stream=event_stream, stream_index=stream_index):
		if not is_stream_active(event_stream):
			CevContext.error("event stream must exist and be active to check if an index has been included in the state")
			return False

		event_stream_dir = CevContext.data_dir() / event_stream

		processed_index = get_stream_index(event_stream, "processed")
		if processed_index is None:
			CevContext.info("state doesn't exist yet (no processed events)")
			return False

		with CevContext.context(processed_index=processed_index):
			if processed_index > stream_index:
				CevContext.info("state is newer than the provided id")
				return True
			else:
				CevContext.info("state has not been processed beyond the provided id")
				return False


def cmd_process(event_stream: str) -> bool:
	with CevContext.context(event_stream=event_stream):
		CevContext.debug("processing event stream")

		if not is_stream_active(event_stream):
			CevContext.error("event stream must be active to process oustanding events")
			return False

		event_stream_dir = CevContext.data_dir() / event_stream
		with CevLock(event_stream_dir, "state"):
			current_index = get_stream_index(event_stream, "current")
			if current_index is None:
				CevContext.debug("no events to process")
				return True

			processed_index = get_stream_index(event_stream, "processed")
			if processed_index is None:
				next_index = 0
			else:
				if current_index == processed_index:
					CevContext.debug("stream fully processed")
					return True

				next_index = processed_index + 1

			# Preload the state schema, we'll check upon load and any produced state against this if its
			# available.
			state_schema = get_schema(event_stream)
			state = read_event_stream_state(event_stream)

			if state:
				if state_schema and not validate_against_schema(state_schema, state):
					CevContext.error("persisted state failed to validate against recorded schema")
					return False
			else:
				state = {}

			CevContext.debug("processing event range", next_index=next_index, current_index=current_index)

			# Note: if processing the stream results in new events to the stream being processed those
			# will not be handled just queued for future runs. This is by design to ensure event streams
			# have bounded execution.
			for idx in range(next_index, current_index + 1):
				events_dir = event_stream_dir / "events"
				event_files = list(events_dir.glob(f"{idx:08x}-*"))

				if not event_files:
					CevContext.error("expected stream index but found gap in sequence", stream_index=f"{idx:08x}")
					return False

				# Technically there could be multiple events with the same index if there is a concurrency
				# bug, the event lock should handle this directly
				event_path = events_dir / event_files[0]
				_, recorded_at, event_name = meta_from_event_file(event_path)

				with CevContext.context(event_id=idx, event_name=event_name):
					CevContext.debug("running stream handler")
					start_time = time.time()
					commands = []

					event_schema = get_schema(event_stream, trigger_name=event_name)
					if not event_schema:
						CevContext.error("unknown event trigger, no schema available")
						return False

					if event_stream == "system-events/scheduled":
						success, state_update = run_scheduled_event_handler(state, event_name, idx, event_path)
						if not success:
							CevContext.error("failed to process scheduled event")
							return False
					elif event_stream == "system-events/extraction":
						success, state_update = run_extraction_event_handler(state, event_name, idx, event_path)
						if not success:
							CevContext.error("failed to process extraction events")
							return False
					else:
						extra_args = [str(event_path)]
						exit_code, state_update, commands = run_trigger(event_stream, "handler", event_name, extra_args)

						if exit_code != 0:
							CevContext.warn("stream handler failed", event_id=idx, exit_code=exit_code)
							return False

					if commands:
						CevContext.debug("stream handler produced commands", cmd_count=len(commands))

						if is_valid_command_list(commands, False):
							if not execute_commands(commands):
								CevContext.error("failed to execute commands")
								return False
						else:
							CevContext.warn("produced commands were invalid")
							return False

					if state_update:
						CevContext.trace("state and schema", state=state_update, schema=state_schema)

						if state_schema and not validate_against_schema(state_schema, state_update):
							CevContext.error("produced state did not validate against the schema")
							return False

						state = state_update
						persist_state(event_stream, idx, state_update)

					set_stream_index(event_stream, "processed", idx)

					duration_ms = int((time.time() - start_time) * 1000)
					CevContext.debug("stream handler succeeded", event_id=idx, duration_ms=duration_ms)

	return True


def cmd_run(event_stream: str, trigger_name: str, extra_args: Optional[List[str]] = None) -> Tuple[bool, Optional[Dict[str, Any]]]:
	if extra_args and extra_args[0] == "--":
		extra_args = extra_args[1:]

	with CevContext.context(event_stream=event_stream, trigger_name=trigger_name, args=extra_args):
		CevContext.debug("running trigger")

		if not cmd_is_clean(event_stream):
			CevContext.debug("processing dirty stream before running trigger")
			if not cmd_process(event_stream):
				CevContext.error("failed to process stream not executing trigger")
				return False, None

		start_time = time.time()
		exit_code, state_update, commands = run_trigger(event_stream, "task", trigger_name, extra_args)
		duration_ms = int((time.time() - start_time) * 1000)

		# Only perform additional processing if the command was successful
		if exit_code != 0:
			CevContext.error("trigger exited with error", duration_ms=duration_ms, exit_code=exit_code)
			return False, None

		CevContext.debug("trigger completed successfully", duration_ms=duration_ms, exit_code=exit_code)

		# Commands are how tasks can modify state in this stream and others, we want to have high
		# assurity that they're all valid before we attempt to apply them to our state.
		if commands:
			CevContext.debug("trigger produced commands", cmd_count=len(commands))

			if is_valid_command_list(commands, False):
				if not execute_commands(commands):
					CevContext.error("failed to execute commands produced by trigger")
					return False, None
			else:
				CevContext.error("trigger produced invalid command list")
				return False, None

		return True, state_update


@dataclass
class UnwatchArgs:
	watcher: str
	target: str
	extractor: str

def cmd_unwatch(args: UnwatchArgs) -> bool:
	if not is_stream_active(args.watcher):
		CevContext.error("watching event stream must exist and be active")
		return False

	if not is_stream_active(args.target):
		CevContext.error("target event stream must exist and be active")
		return False

	if not is_valid_event_name(args.extractor):
		CevContext.error("invalid extration trigger name", extractor=args.extractor)
		return False

	with CevContext.context(watcher=args.watcher, target=args.target, extractor=args.extractor):
		watchers = get_watchers(args.target)

		current_extractors = watchers.get(args.watcher, [])
		remaining_extractors = [extr for extr in current_extractors if extr != args.extractor]
		watchers[args.watcher] = remaining_extractors

		set_watchers(args.target, watchers)

		return True



@dataclass
class WatchArgs:
	watcher: str
	target: str
	extractor: str

def cmd_watch(args: WatchArgs) -> bool:
	if not is_stream_active(args.watcher):
		CevContext.error("watching event stream must exist and be active")
		return False

	if not is_stream_active(args.target):
		CevContext.error("target event stream must exist and be active")
		return False

	if not is_valid_event_name(args.extractor):
		CevContext.error("invalid extration trigger name", extractor=args.extractor)
		return False

	with CevContext.context(watcher=args.watcher, target=args.target, extractor=args.extractor):
		watchers = get_watchers(args.target)
		if not watchers.get(args.watcher):
			watchers[args.watcher] = []

		watchers[args.watcher].append(args.extractor)
		set_watchers(args.target, watchers)

		# We intentionally do not process the target event stream, if it hasn't already been processed or
		# there are no events to process we produce a warning but this isn't a failure. When the state
		# eventually does get produced by the target stream it will automatically enqueue one of these
		# events.
		latest_snapshot = get_latest_state_file(args.target)
		if latest_snapshot:
			schedule_extraction(args.watcher, args.target, args.extractor, latest_snapshot.name)
		else:
			CevContext.warn("no state for target stream, not automatically including an extraction event")

		return True


def main():
	parser = argparse.ArgumentParser(description="Common Event system")

	# Global arguments
	parser.add_argument("--cfg-dir", help="Override default config directory")
	parser.add_argument("-d", "--data-dir", help="Override default data directory")
	parser.add_argument("-l", "--log-level", default="info", choices=["trace", "debug", "info", "warn", "error", "fatal"], help="Set minimum log level (default: info)")

	subparsers = parser.add_subparsers(dest="command", required=True)

	# Command specific arguments
	add_event_parser = subparsers.add_parser("add-event", help="Add an event to a stream")
	add_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	add_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	add_event_parser.add_argument("-f", "--file", help="Event JSON file path")
	add_event_parser.add_argument("-p", "--process", action="store_true", help="Process stream after adding event")

	alias_trigger_parser = subparsers.add_parser("alias-trigger", help="Create an alias of an existing trigger")
	alias_trigger_parser.add_argument("-s", "--source-stream", required=True, help="Source event stream name")
	alias_trigger_parser.add_argument("-n", "--source-name", required=True, help="Source trigger name")
	alias_trigger_parser.add_argument("-k", "--source-kind", default="handler", help="Source trigger kind (handler, extractor, task)")
	alias_trigger_parser.add_argument("-d", "--dest-stream", help="Destination event stream name (defaults to source stream)")
	alias_trigger_parser.add_argument("-m", "--dest-name", help="Destination trigger name (defaults to source name)")
	alias_trigger_parser.add_argument("-j", "--dest-kind", help="Destination trigger kind (defaults to source kind)")

	create_event_parser = subparsers.add_parser("create-event", help="Create an event stub")
	create_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	create_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	create_event_parser.add_argument("-f", "--file", help="Output file path")

	define_event_parser = subparsers.add_parser("define-event", help="Define an event schema")
	define_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	define_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	define_event_parser.add_argument("-f", "--file", help="JSON schema file path")

	define_trigger_parser = subparsers.add_parser("define-trigger", help="Define a trigger")
	define_trigger_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	define_trigger_parser.add_argument("-n", "--name", required=True, help="Trigger name")
	define_trigger_parser.add_argument("-t", "--trigger", required=True, help="Trigger executable path")
	define_trigger_parser.add_argument("-k", "--kind", required=True, help="Trigger kind (handler, extractor, task)")
	define_trigger_parser.add_argument("-c", "--copy", action="store_true", help="Copy the provided executable into the data directory (will symlink if not copied)")

	is_clean_parser = subparsers.add_parser("is-clean", help="Verify all events are in a stream have been processed")
	is_clean_parser.add_argument("-s", "--stream", required=True, help="Event stream name")

	is_newer_parser = subparsers.add_parser("is-newer", help="Check if a stream's latest state snapshot is newer than provided index")
	is_newer_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	is_newer_parser.add_argument("-i", "--index", required=True, help="Stream index to compare against (in base16)")

	subparsers.add_parser("info", help="Display system summary information")

	process_parser = subparsers.add_parser("process", help="Process events in a stream")
	process_parser.add_argument("-s", "--stream", required=True, help="Event stream name")

	run_parser = subparsers.add_parser("run", help="Run a specific task in an event stream")
	run_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	run_parser.add_argument("-n", "--name", required=True, help="Task trigger name")
	run_parser.add_argument("extra_args", nargs=argparse.REMAINDER, help="Additional arguments to pass to the trigger (after --)")

	secrets_parser = subparsers.add_parser("secrets", help="Manage encrypted secrets")
	secrets_subparsers = secrets_parser.add_subparsers(dest="secrets_command", required=True)

	set_secret_parser = secrets_subparsers.add_parser("set", help="Set a secret value")
	set_secret_parser.add_argument("-n", "--name", required=True, help="Secret name")
	set_secret_parser.add_argument("-v", "--value", required=True, help="Secret value (omit to read from stdin)")

	grant_parser = secrets_subparsers.add_parser("grant", help="Grant a trigger access to a secret")
	grant_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	grant_parser.add_argument("-k", "--kind", required=True, help="Trigger kind")
	grant_parser.add_argument("-t", "--trigger", required=True, help="Trigger name")
	grant_parser.add_argument("-n", "--name", required=True, help="Secret name")

	revoke_parser = secrets_subparsers.add_parser("revoke", help="Revoke a trigger's access to a secret")
	revoke_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	revoke_parser.add_argument("-k", "--kind", required=True, help="Trigger kind")
	revoke_parser.add_argument("-t", "--trigger", required=True, help="Trigger name")
	revoke_parser.add_argument("-n", "--name", required=True, help="Secret name")

	list_parser = secrets_subparsers.add_parser("list", help="List available secrets")

	unwatch_parser = subparsers.add_parser("unwatch", help="Remove a watched stream relationship")
	unwatch_parser.add_argument("-w", "--watcher", required=True, help="Watcher stream name")
	unwatch_parser.add_argument("-t", "--target", required=True, help="Target stream to watch")
	unwatch_parser.add_argument("-e", "--extractor", required=True, help="Name of state extraction trigger")

	watch_parser = subparsers.add_parser("watch", help="Establish a watched stream relationship")
	watch_parser.add_argument("-w", "--watcher", required=True, help="Watcher stream name")
	watch_parser.add_argument("-t", "--target", required=True, help="Target stream to watch")
	watch_parser.add_argument("-e", "--extractor", required=True, help="Name of state extraction trigger")

	args = parser.parse_args()
	cfg_dir = get_config_directory(args.cfg_dir)
	data_dir = get_data_directory(args.data_dir)

	CevContext.init(cfg_dir, data_dir, args.log_level)
	CevContext.debug("starting new session", command=args.command, version=VERSION)

	if args.command == "add-event":
		event_path = Path(args.file).expanduser().resolve()
		if not event_path.exists():
			CevContext.fatal("provided file doesn't exist", event_path=event_path)

		payload = event_path.read_text().strip()
		cmd_add_event(AddEventArgs(
			event_stream=args.stream,
			event_name=args.name,
			payload=payload,
		))

		# Process event immediately if requested by the caller
		if args.process:
			cmd_process(args.stream)
	elif args.command == "alias-trigger":
		cmd_alias_trigger(AliasTriggerArgs(
			source_stream=args.source_stream,
			source_name=args.source_name,
			source_kind=args.source_kind,
			dest_stream=args.dest_stream,
			dest_name=args.dest_name,
			dest_kind=args.dest_kind
		))
	elif args.command == "create-event":
		cmd_create_event(args.stream, args.name, args.file)
	elif args.command == "define-event":
		schema_path = Path(args.file).expanduser().resolve()

		schema = read_json_file(schema_path)
		if not schema:
			CevContext.error("unable to access provided schema", schema_path=schema_path)

		cmd_define_event(DefineEventArgs(
			event_stream=args.stream,
			event_name=args.name,
			schema=schema,
		))
	elif args.command == "define-trigger":
		trigger_path = Path(args.trigger).expanduser().resolve()

		cmd_define_trigger(DefineTriggerArgs(
			event_stream=args.stream,
			trigger_name=args.name,
			trigger_kind=args.kind,
			trigger_path=trigger_path,
			copy_trigger=args.copy,
		))
	elif args.command == "info":
		cmd_info()
	elif args.command == "is-clean":
		if cmd_is_clean(args.stream):
			sys.exit(0)
		else:
			sys.exit(1)
	elif args.command == "is-newer":
		try:
			stream_index = int(args.index, 16)
		except ValueError:
			# This one however is a user error and may not be the correct response but its the best we can do
			CevContext.fatal("invalid stream index format (should be base16)")

		if cmd_is_newer(args.stream, stream_index):
			sys.exit(0)
		else:
			sys.exit(1)
	elif args.command == "process":
		cmd_process(args.stream)
	elif args.command == "run":
		cmd_run(args.stream, args.name, args.extra_args)
	elif args.command == "secrets":
		secrets = CevContext.secrets()

		if args.secrets_command == "set":
			value = args.value
			if value is None:
				CevContext.fatal("value from stdin not yet implemented")

			if secrets.set_secret(args.name, value):
				CevContext.info("secret set successfully", name=args.name)

		elif args.secrets_command == "grant":
			if secrets.grant_access(args.stream, args.kind, args.trigger, args.name):
				CevContext.info("secret access granted", stream=args.stream, trigger=args.trigger, secret=args.name)

		elif args.secrets_command == "revoke":
			if secrets.revoke_access(args.stream, args.kind, args.trigger, args.name):
				CevContext.info("secret access revoked", stream=args.stream, trigger=args.trigger, secret=args.name)

		elif args.secrets_command == "list":
			secrets = secrets.list_secrets()
			if not secrets:
				CevContext.info("no secrets currently stored")

			for name, triggers in secrets:
				CevContext.info("secret", name=name, triggers=triggers)

	elif args.command == "watch":
		cmd_watch(WatchArgs(
			watcher=args.watcher,
			target=args.target,
			extractor=args.extractor,
		))
	elif args.command == "unwatch":
		cmd_unwatch(UnwatchArgs(
			watcher_stream=args.watcher,
			target_stream=args.target,
			extractor=args.extractor,
		))

if __name__ != "__main__":
	print("utility is not available as a library", file=sys.stderr)
	sys.exit(1)

# Explicit exit is important to ensure we don't try and execute the embedded schemas
sys.exit(main())

__EMBEDDED_SCHEMAS__
# [system-events-extraction-extract-task-schema]
{
	"type": "object",
	"properties": {
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"},
		"new_state_name": {"type": "string"},
		"previous_state_name": {"type": "string"}
	},
	"required": ["watcher", "target", "extractor", "new_state_name"]
}

# [system-events-extraction-complete-task-schema]
{
	"type": "object",
	"properties": {
		"task_id": {"type": "string"},
		"ctx_id": {"type": "string"},
		"generated_events": {"type": "integer"}
	},
	"required": ["task_id", "ctx_id", "generated_events"]
}

# [system-events-extraction-state-schema]
{
	"type": "object",
	"properties": {
		"tasks": {
			"type": "array",
			"items": {
				"type": "object",
				"properties": {
					"task_id": {"type": "string"},
					"watcher": {"type": "string"},
					"target": {"type": "string"},
					"extractor": {"type": "string"},
					"new_state_name": {"type": "string"},
					"previous_state_name": {"type": "string"}
				},
				"required": ["task_id", "watcher", "target", "extractor", "new_state_name"]
			}
		}
	},
	"required": ["tasks"]
}

# [system-events-scheduled-complete-task-schema]
{
	"type": "object",
	"properties": {
		"task_id": {"type": "string"},
		"ctx_id": {"type": "string"},
		"status": {"type": "string", "enum": ["success", "failure"]}
	},
	"required": ["task_id", "ctx_id", "status"]
}

# [system-events-scheduled-state-schema]
{
	"type": "object",
	"properties": {
		"tasks": {
			"type": "array",
			"items": {
				"type": "object",
				"properties": {
					"task_id": {"type": "string"},
					"event_stream": {"type": "string"},
					"trigger_name": {"type": "string"},
					"extra_args": { "type": "array", "items": { "type": "string" } },
					"scheduled_for": {"type": "string", "format": "date-time"}
				},
				"required": ["task_id", "event_stream", "trigger_name", "extra_args", "scheduled_for"]
			}
		}
	},
	"required": ["tasks"]
}

# [system-events-scheduled-schedule-task-schema]
{
	"type": "object",
	"properties": {
		"event_stream": {"type": "string"},
		"trigger_name": {"type": "string"},
		"extra_args": { "type": "array", "items": { "type": "string" } },
		"scheduled_for": {"type": "string", "format": "date-time"}
	},
	"required": ["event_stream", "trigger_name", "scheduled_for"]
}

# [cmd-add-event-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["add-event"]},
		"event_stream": {"type": "string"},
		"event_name": {"type": "string"},
		"payload": {"type": "object"}
	},
	"required": ["command", "event_stream", "event_name", "payload"]
}

# [cmd-alias-trigger-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["alias-trigger"]},
		"source_stream": {"type": "string"},
		"source_name": {"type": "string"},
		"source_kind": {"type": "string", "enum": ["handler", "extractor", "task"]},
		"dest_stream": {"type": "string"},
		"dest_name": {"type": "string"},
		"dest_kind": {"type": "string", "enum": ["handler", "extractor", "task"]}
	},
	"required": ["command", "source_stream", "source_name"]
}

# [cmd-define-event-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["define-event"]},
		"event_stream": {"type": "string"},
		"event_name": {"type": "string"},
		"schema": {"type": "object"}
	},
	"required": ["command", "event_stream", "event_name", "schema"]
}

# [cmd-define-trigger-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["define-trigger"]},
		"event_stream": {"type": "string"},
		"trigger_name": {"type": "string"},
		"trigger_kind": {"type": "string"},
		"trigger_path": {"type": "string"},
		"copy_trigger": {"type": "bool"}
	},
	"required": ["command", "event_stream", "trigger_name", "trigger_kind", "trigger_path"]
}

# [cmd-unwatch-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["unwatch"]},
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"}
	},
	"required": ["command", "watcher", "target", "extractor"]
}

# [cmd-watch-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["watch"]},
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"}
	},
	"required": ["command", "watcher", "target", "extractor"]
}

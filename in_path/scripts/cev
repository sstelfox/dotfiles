#!/usr/bin/env -S uv run -q -s

# vim: set ts=4 sw=4 noexpandtab ai :

# /// script
# requires-python = ">=3.12"
# dependencies = ["jsonschema>=4.23"]
# ///

import argparse
import fcntl
import json
import os
import re
import signal
import subprocess
import sys
import time

from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import jsonschema

VERSION = "0.4.0"

DEFAULT_SCHEDULE_TIME_LIMIT_SECS = 4 * 60 + 30


class LogLevel(str, Enum):
	TRACE = "trace"
	DEBUG = "debug"
	INFO = "info"
	WARN = "warn"
	ERROR = "error"
	FATAL = "fatal"
	INVALID = "invalid"

	@classmethod
	def from_string(cls, value: str) -> "LogLevel":
		try:
			return cls(value)
		except ValueError:
			return cls.INVALID


class CevLogger:
	def __init__(self, log_file: Path, verbose: bool = False):
		self.log_file = log_file
		self.verbose = verbose or os.environ.get("CEV_VERBOSE") is not None
		self.log_file.parent.mkdir(parents=True, exist_ok=True)

	def log(self, level: LogLevel, message: str, **kwargs):
		timestamp = datetime.now(timezone.utc).isoformat()

		# Prevent these from being injected, we always want to be the authority of the timestamp, the
		# others must be provided through the explicit arguments to prevent bad data from obscurring
		# messages.
		reserved_keys = {"timestamp", "level", "message"}
		safe_kwargs = {k: v for k, v in kwargs.items() if k not in reserved_keys}

		log_entry = {
			"timestamp": timestamp,
			"level": level.value,
			"message": message,
			**safe_kwargs
		}

		if self.verbose or level != LogLevel.TRACE:
			with open(self.log_file, "a") as f:
				f.write(json.dumps(log_entry) + "\n")

		if self.verbose or level not in {LogLevel.TRACE, LogLevel.DEBUG}:
			system_output = json.dumps(log_entry)
			sys.stderr.write(f"{system_output}\n")

	def trace(self, message: str, **kwargs):
		self.log(LogLevel.TRACE, message, **kwargs)

	def debug(self, message: str, **kwargs):
		self.log(LogLevel.DEBUG, message, **kwargs)

	def info(self, message: str, **kwargs):
		self.log(LogLevel.INFO, message, **kwargs)

	def warn(self, message: str, **kwargs):
		self.log(LogLevel.WARN, message, **kwargs)

	def error(self, message: str, **kwargs):
		self.log(LogLevel.ERROR, message, **kwargs)

	def fatal(self, message: str, **kwargs):
		self.log(LogLevel.FATAL, message, **kwargs)
		sys.exit(127)


_CURRENT_CONTEXT = {}

class CevContext:
	@staticmethod
	def clear(key: str) -> Optional[Any]:
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		if key == "ctx_id":
			return None

		return _CURRENT_CONTEXT["attrs"].pop(key, None)

	@staticmethod
	def ctx_id() -> str:
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return _CURRENT_CONTEXT["attrs"]["ctx_id"]

	@staticmethod
	def data_dir():
		if "data_dir" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		return _CURRENT_CONTEXT["data_dir"]

	@staticmethod
	def init(data_dir: Path, verbose: bool = False):
		global _CURRENT_CONTEXT
		log_file = data_dir / "system.log"

		# Generate a random ID to differentiate between different runs of the tool
		import random
		import string

		chars = string.ascii_lowercase + string.digits
		ctx_id = ''.join(random.choices(chars, k=8))

		_CURRENT_CONTEXT = {
			"logger": CevLogger(log_file, verbose),
			"data_dir": data_dir,
			"attrs": {
				"ctx_id": ctx_id
			}
		}

	@staticmethod
	def log(level, message, **kwargs):
		if "logger" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		# Merge context attributes with kwargs (kwargs take precedence)
		merged_kwargs = _CURRENT_CONTEXT["attrs"].copy()
		merged_kwargs.update(kwargs)

		_CURRENT_CONTEXT["logger"].log(level, message, **merged_kwargs)

	@staticmethod
	def set(key: str, value: Any):
		if "attrs" not in _CURRENT_CONTEXT:
			raise RuntimeError("CevContext not initialized")

		if key == "ctx_id":
			return

		_CURRENT_CONTEXT["attrs"][key] = value

	@staticmethod
	def context(**kwargs):
		return _TempContextManager(kwargs)

	@staticmethod
	def trace(message, **kwargs):
		CevContext.log(LogLevel.TRACE, message, **kwargs)

	@staticmethod
	def debug(message, **kwargs):
		CevContext.log(LogLevel.DEBUG, message, **kwargs)

	@staticmethod
	def info(message, **kwargs):
		CevContext.log(LogLevel.INFO, message, **kwargs)

	@staticmethod
	def warn(message, **kwargs):
		CevContext.log(LogLevel.WARN, message, **kwargs)

	@staticmethod
	def error(message, **kwargs):
		CevContext.log(LogLevel.ERROR, message, **kwargs)

	@staticmethod
	def fatal(message, **kwargs):
		CevContext.log(LogLevel.FATAL, message, **kwargs)
		sys.exit(127)


class _TempContextManager:
	def __init__(self, kwargs):
		self.kwargs = kwargs
		self.previous = {}

	def __enter__(self):
		for key, value in self.kwargs.items():
			self.previous[key] = _CURRENT_CONTEXT["attrs"].get(key)
			CevContext.set(key, value)

		return self

	def __exit__(self, exc_type, exc_val, exc_tb):
		for key, value in self.previous.items():
			if value is None:
				CevContext.clear(key)
			else:
				CevContext.set(key, value)


class CevLock:
	def __init__(self, lock_dir: Path, lock_type: str = "state"):
		valid_lock_types = {"config", "event", "extraction", "state"}
		if lock_type not in valid_lock_types:
			CevContext.fatal("unknown lock type requested", lock_type=lock_type)

		if lock_type in {"config", "event", "extraction"}:
			self.wait_timeout_seconds = 5
		else:
			self.wait_timeout_seconds = 45

		self.lock_file = lock_dir / f"{lock_type}.flock"
		self.lock_type = lock_type
		self.lock_fd = None

	def __enter__(self):
		self.lock_file.parent.mkdir(parents=True, exist_ok=True)
		self.lock_fd = open(self.lock_file, "w+")
		lock_acquired = False

		CevContext.trace("attempting to retrieve lock")

		try:
			fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
			lock_acquired = True

			CevContext.set(f"{self.lock_type}_lock", True)
			CevContext.trace("lock acquired")
		except BlockingIOError:
			CevContext.trace("waiting for held lock", lock_type=self.lock_type)
			start_time = time.time()

			while time.time() - start_time < self.wait_timeout_seconds:
				try:
					fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)

					CevContext.set(f"{self.lock_type}_lock", True)
					CevContext.trace("acquired lock after wait", lock_type=self.lock_type, wait_duration=time.time() - start_time)

					lock_acquired = True
					break
				except BlockingIOError:
					time.sleep(0.1)

		if not lock_acquired:
			CevContext.fatal("could not acquire lock within timeout", lock_type=self.lock_type, timeout=self.wait_timeout_seconds, path=self.lock_file)

		# Write PID to the file after acquiring the lock, can be useful in diagnostics
		self.lock_fd.write(str(os.getpid()))
		self.lock_fd.flush()

		os.fsync(self.lock_fd.fileno())

		return self

	def __exit__(self, exc_type, exc_val, exc_tb):
		try:
			if self.lock_fd:
				fcntl.flock(self.lock_fd, fcntl.LOCK_UN)
				self.lock_fd.close()
		finally:
			self.lock_fd = None
			CevContext.clear(f"{self.lock_type}_lock")
			CevContext.trace("lock released", **{f"{self.lock_type}_lock": False})


def get_data_directory(data_dir: Optional[Path]) -> Path:
	if data_dir:
		return Path(data_dir).expanduser().resolve()

	env_data_dir = os.environ.get("CEV_DATA_DIR")
	if env_data_dir:
		return Path(env_data_dir).expanduser().resolve()

	xdg_data_home = os.environ.get("XDG_DATA_HOME")
	if xdg_data_home:
		return Path(xdg_data_home) / "cev"

	return Path.home() / ".local" / "share" / "cev"


def execute_commands(commands: List[Dict[str, Any]]) -> bool:
	CevContext.debug("executing event generated commands", count=len(commands))

	for cmd in commands:
		command_type = cmd.pop("command")
		if command_type == "add-event":
			return cmd_add_event(AddEventArgs(**cmd))
		else:
			# todo: implement and make a test example for the other commands
			CevContext.error("command type not implemented", command_type=command_type)
			return False


def execute_scheduled_events(extra_args: Optional[List[str]]) -> bool:
	CevContext.debug("executing scheduled events using built-in handler")

	runtime_limit = DEFAULT_SCHEDULE_TIME_LIMIT_SECS
	if extra_args:
		parser = argparse.ArgumentParser(description="Common Event system - Scheduled Event Execution")
		parser.add_argument("-t", "--limit-time", type=int, help="Specify a custom time limit (in seconds) for processing all outstanding tasks during this run. Specifying a time limit of 45 seconds or less will prevent warning signals from triggering")

		args = parser.parse_args(extra_args)
		if args.limit_time:
			CevContext.debug("setting user specified time limit for event processing", time_limit=args.limit_time)
			runtime_limit = args.limit_time

	event_stream = "system-events/scheduled"
	event_stream_dir = CevContext.data_dir() / event_stream

	with CevLock(event_stream_dir, "state"):
		state = read_event_stream_state(event_stream)
		if not state:
			CevContext.debug("no scheduled tasks have been processed; scheduled tasks execution complete")
			return True

		now_dt = datetime.now(timezone.utc)
		now = now_dt.isoformat()

		due_tasks = []
		for task in state["scheduled_tasks"]:
			scheduled_dt = datetime.fromisoformat(task["scheduled_for"])
			if scheduled_dt <= now_dt:
				due_tasks.append(task)

		due_tasks.sort(key=lambda x: x.get("scheduled_for"))

		if len(due_tasks) == 0:
			CevContext.debug("no tasks due for execution; scheduled task execution complete")
			return True

		CevContext.trace("scheduled tasks scheduled for execution", tasks_ready=len(due_tasks))

		start_time = time.time()
		executed_task_count = 0
		failed_streams = set()

		for task in due_tasks:
			time_elapsed = time.time() - start_time
			time_remaining = runtime_limit - time_elapsed

			if time_remaining < 60:
				CevContext.warn("insufficient buffer time remaining, skipping remaining tasks")
				break

			if task["event_stream"] in failed_streams:
				CevContext.debug("skipping ready task in failed stream")
				continue

			success, _ = cmd_run(task["event_stream"], task["trigger_name"], task.get("extra_args"))
			if not success:
				failed_streams.add(task["event_stream"])

			status = "success" if success else "failure"
			cmd_add_event(AddEventArgs(
				event_stream="system-events/scheduled",
				event_name="complete-task",
				payload={ "task_id": task["task_id"], "ctx_id": CevContext.ctx_id(), "status": status }
			))

			executed_task_count += 1
			CevContext.trace("completed task", task_id=task["task_id"], event_stream=event["event_stream"], trigger_name=task["trigger_name"])

	# Process any emitted events to ensure the resulting state file is inspectable
	if not cmd_process("system-events/scheduled"):
		CevContext.error("failed to process scheduled event queue after executing outstanding tasks")
		return False

	remaining = len(due_tasks) - executed_task_count
	CevContext.debug("scheduled task execution complete", executed=executed_task_count, remaining=remaining)


def process_stderr_buffer(buffer: str, output_commands: List[Dict[str, Any]]) -> str:
	if '\n' in buffer:
		lines = buffer.split('\n')
		remaining = lines.pop()

		for line in lines:
			command = filter_trigger_command(line)
			if command:
				output_commands.append(command)

		return remaining

	return buffer


def filter_trigger_command(line: str) -> Optional[Dict[str, Any]]:
	if not line.strip():
		return None

	try:
		data = json.loads(line)
	except json.JSONDecodeError:
		# It's not JSON so its probably a raw runtime error. This is truly exceptional output we can't
		# handle and probably the result of a problem in the user's script. These are frequently
		# multi-line which will come in all at once. We bypass the standard logging message and dump it
		# straight to stderr so a user running it interactively can see the messages without them being
		# obscurred by the json formatting.
		#
		# In the future I may want to capture all of these in a diagnostic log tied to the context ID but
		# this works surprisingly well.
		sys.stderr.write(f"{line}\n")
		return None

	if not isinstance(data, dict):
		CevContext.warn("non-command json output on stderr", stderr=line)
		return None

	if "command" in data:
		return data

	# Try and treat the line as a log message which only requires a message key
	message = data.pop("message", None)
	if message is None:
		CevContext.warn("trigger produced JSON output but was neither command nor log", content=data)
		return None

	log_level = LogLevel.from_string(data.pop("level", "info"))
	CevContext.log(log_level, message, **data)


def generate_stub_from_schema(schema: Dict[str, Any]) -> Optional[Any]:
	schema_type = schema.get("type")
	if not schema_type:
		return {}

	if schema_type == "object":
		result = {}

		for prop_name, prop_schema in schema.get("properties", {}).items():
			result[prop_name] = generate_stub_from_schema(prop_schema)

		return result
	elif schema_type == "array":
		if "items" in schema:
			return [generate_stub_from_schema(schema["items"])]

		return []
	elif schema_type == "string":
		if schema.get("format") == "date-time":
			return datetime.now(timezone.utc).isoformat()

		return ""
	elif schema_type == "boolean":
		return False
	elif schema_type == "number":
		return 0.0
	elif schema_type == "integer":
		return 0
	elif schema_type == "null":
		return None

	CevContext.warn("unrecognized schema type, leaving as null", schema_type=schema_type)
	return None


def get_active_schema_versions(event_stream: str) -> Dict[str, str]:
	active_versions = {}

	active_version_path = CevContext.data_dir() / event_stream / "schemas" / "active_versions"
	if not active_version_path.exists():
		CevContext.trace("no active schema versions for event stream", event_stream=event_stream)
		return active_versions

	with open(active_version_path, "r") as f:
		for line in f:
			line = line.strip()
			if line:
				active_versions[line.split('.')[0]] = line

	return active_versions


def get_schema(event_stream: str, trigger_name: Optional[str] = None) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking up schema", event_stream=event_stream, trigger_name=trigger_name)

	if event_stream.startswith("system-events"):
		if not trigger_name:
			trigger_name = "state"

		event_stream = event_stream.replace("/", "-")

		return get_embedded_schema(f"{event_stream}-{trigger_name}-schema")
	else:
		if not trigger_name:
			return None

		return get_user_schema(event_stream, trigger_name)

def get_embedded_schema(key: str) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking for embedded schema", key=key)

	# We know this script is self-executing as we've explicitly restricted it to only allow
	# self-execution
	script_path = Path(sys.argv[0])
	with open(script_path, "r") as f:
		script_content = f.read()

	if "__EMBEDDED_SCHEMAS__" not in script_content:
		CevContext.fatal("missing schema section in script")

	schema_section = script_content.split("__EMBEDDED_SCHEMAS__", 1)[1]

	current_key = None
	content_lines = []
	schema_text = None

	# Parse the data section to find the requested key
	for line in schema_section.split("\n"):
		if line.startswith("# [") and line.endswith("]"):
			# We found the next defined schema
			if current_key == key and content_lines:
				schema_text = "\n".join(content_lines)

			# Collect the lines until we reach the next section so we have all its contents
			current_key = line[3:-1]  # Remove "# [" and "]"
			content_lines = []
		elif current_key == key:
			content_lines.append(line)

	# The last schema was the one we were looking for
	if current_key == key and content_lines:
		schema_text = "\n".join(content_lines)

	if not schema_text:
		return None

	try:
		return json.loads(schema_text)
	except json.JSONDecodeError as e:
		CevContext.fatal("failed to decode built-in schema", error=str(e), schema_key=key)


def get_latest_state_file(event_stream: str) -> Optional[Path]:
	state_dir = CevContext.data_dir() / event_stream / "state"
	if not state_dir.exists():
		return None

	# Might be more efficient to read the processed index and find the matching state... but that
	# won't work for events that don't modify state...
	snapshots = sorted(state_dir.glob("*.state.json"), key=lambda x: str(x))
	if not snapshots:
		return None

	return snapshots[-1]


def get_stream_index(event_stream: str, index_kind: str) -> Optional[int]:
	index_file = CevContext.data_dir() / event_stream / f"{index_kind}.idx"
	if not index_file.exists():
		return None

	return int(index_file.read_text().strip(), 16)


def get_user_schema(event_stream: str, event_name: str) -> Optional[Dict[str, Any]]:
	CevContext.debug("looking for user schema")

	active_versions = get_active_schema_versions(event_stream)

	schema_version = active_versions.get(event_name)
	if not schema_version:
		CevContext.error("event name doesn't have an active schema")
		return None

	schema_dir = CevContext.data_dir() / event_stream / "schemas"
	schema_path = schema_dir / f"{schema_version}.json"

	return read_json_file(schema_path)


#def get_watchers(event_stream_dir: Path) -> List[Tuple[str, str]]:
#    watchers_file = event_stream_dir / "watchers"
#    watchers = []
#
#    if not watchers_file.exists():
#        return watchers
#
#    with open(watchers_file, "r") as f:
#        for line in f:
#            entry = line.strip()
#            if entry and ":" in entry:
#                watcher, trigger = entry.split(":", 1)
#                watchers.append((watcher, trigger))
#
#    return watchers


#def get_watched_streams_info(event_stream_dir: Path) -> List[str]:
#    watched_streams_file = event_stream_dir / "watched_streams"
#    watched_streams = []
#
#    if not watched_streams_file.exists():
#        return watched_streams
#
#    with open(watched_streams_file, "r") as f:
#        for line in f:
#            stream = line.strip().split(':')[0]
#            if stream and stream not in watched_streams:
#                watched_streams.append(stream)
#
#    return watched_streams


def meta_from_event_file(file: Path) -> Tuple[Optional[int], Optional[str], Optional[str]]:
	match = re.match(r'^([0-9a-f]{8})-(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+[\-+]\d{2}:\d{2})-([a-z0-9\-]{3,64})\.evt$', file.name)

	if match:
		event_id_hex, recorded_at, trigger_name = match.groups()
		event_id = int(event_id_hex, 16)
		return event_id, recorded_at, trigger_name
	else:
		return None, None, None


def process_extraction_events() -> bool:
	CevContext.debug("processing extraction events using built-in handler")
	CevContext.error("process_extraction_events not yet implemented")
	return False


def process_scheduled_event_stream() -> bool:
	event_stream = "system-events/scheduled"
	event_stream_dir = CevContext.data_dir() / event_stream

	with CevLock(event_stream_dir, "state"):
		current_index = get_stream_index(event_stream, "current")
		if current_index is None:
			CevContext.info("no events to process")
			return True

		processed_index = get_stream_index(event_stream, "processed")
		if processed_index is None:
			next_index = 0
		else:
			next_index = processed_index + 1

		# Preload the state schema, we'll check upon load and any produced state against this if its
		# available.
		state_schema = get_schema(event_stream)
		state = read_event_stream_state(event_stream)

		if state:
			if state_schema and not validate_against_schema(state_schema, state):
				CevContext.error("persisted state failed to validate against recorded schema")
				return False
		else:
			state = generate_stub_from_schema(schema)

		CevContext.debug("processing event range", next_index=next_index, current_index=current_index)

		# Note: if processing the stream results in new events to the stream being processed those
		# will not be handled just queued for future runs. This is by design to ensure event streams
		# have bounded execution.
		for idx in range(next_index, current_index + 1):
			events_dir = event_stream_dir / "events"
			event_files = list(events_dir.glob(f"{idx:08x}-*"))

			if not event_files:
				CevContext.error("expected stream index but found gap in sequence", stream_index=f"{idx:08x}")
				return False

			# Technically there could be multiple events with the same index if there is a concurrency
			# bug, the event lock should handle this directly
			event_path = events_dir / event_files[0]
			_, recorded_at, event_name = meta_from_event_file(event_path)

			with CevContext.context(event_id=idx, trigger_name=trigger_name):
				start_time = time.time()
				state_update = None

				trigger_schema = get_schema(event_stream, trigger_name=trigger_name)
				if not trigger_schema:
					CevContext.error("unknown event trigger, no schema available")
					return False

				# BEGIN SCHEDULING SPECIFIC HANDLING
				event = read_json_file(event_path)
				if not event:
					CevContext.error("unable to load persisted event")
					return False

				if trigger_name == "schedule-task":
					task_id = f"{idx:08x}"
					CevContext.debug("scheduling task", task_id=task_id, scheduled_for=event["scheduled_for"])

					state_update = state.copy()
					state_update["scheduled_tasks"].append({
						"task_id": task_id,
						"event_stream": event["event_stream"],
						"trigger_name": event["trigger_name"],
						"extra_args": event.get("extra_args", []),
						"scheduled_for": event["scheduled_for"],
					})
				elif trigger_name == "complete-task":
					CevContext.debug("completing task", task_id=event["task_id"], status=event["status"])

					remaining_tasks = [task for task in state["scheduled_tasks"] if task["task_id"] != event["task_id"]]

					state_update = state.copy()
					state_update = {
						"scheduled_tasks": remaining_tasks
					}
				else:
					CevContext.error("unknown trigger for stream")
					return False
				# END SCHEDULING SPECIFIC HANDLING

				if state_update:
					if state_schema and not validate_against_schema(state_schema, state_update):
						CevContext.error("produced state did not validate against the schema")
						CevContext.trace("incompatible state and schema", state=state_update, schema=state_schema)
						return False

					persist_state(event_stream, idx, state_update)

				set_stream_index(event_stream, "processed", idx)

				duration_ms = int((time.time() - start_time) * 1000)
				CevContext.debug("stream handler succeeded", event_id=idx, duration_ms=duration_ms)

	return True


@dataclass
class StateMetadata:
	event_stream: str
	stream_idx: str
	timestamp: str
	schema_versions: Dict[str, str] = field(default_factory=dict)
	watched_streams: List[str] = field(default_factory=list)

@dataclass
class State:
	metadata: StateMetadata
	state: Dict[str, Any]

def persist_state(event_stream: str, stream_idx: int, state_payload: Dict[str, Any]):
	metadata = StateMetadata(
			event_stream=event_stream,
			stream_idx=f"{stream_idx:08x}",
			timestamp=datetime.now(timezone.utc).isoformat()
	)

	active_schema_versions = get_active_schema_versions(event_stream)
	if active_schema_versions:
		metadata.schema_versions = active_schema_versions

	full_state = State(metadata=metadata, state=state_payload)

	state_dir = CevContext.data_dir() / event_stream / "state"
	state_dir.mkdir(parents=True, exist_ok=True)

	state_path = state_dir / f"{stream_idx:08x}-{metadata.timestamp}.state.json"
	with open(state_path, "w") as f:
		json.dump(asdict(full_state), f, indent=2)

	# todo: read and record watched streams
	CevContext.warn("watched stream state announcements not yet implemented")

	# Check for watchers interested in this stream
	#watchers = get_watchers(event_stream_dir)
	#if watchers:
	#	for watcher_stream, extraction_trigger in watchers:
	#		schedule_extraction(watcher, target, extractor, state)

	CevContext.debug("persisted event stream state")


def read_event_stream_state(event_stream: str) -> Tuple[Optional[str], Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
	latest_snapshot_path = get_latest_state_file(event_stream)
	if not latest_snapshot_path:
		CevContext.debug("no state available for event stream")
		return None

	envelope = read_json_file(latest_snapshot_path)
	if not envelope:
		CevContext.error("unable to load state file")
		return None

	return envelope.get("state")


def read_json_file(file_path: Path) -> Optional[Dict[str, Any]]:
	try:
		with open(file_path, "r") as f:
			return json.load(f)
	except (json.JSONDecodeError, FileNotFoundError) as e:
		CevContext.fatal("failed to load data file", error=str(e))


def run_trigger(event_stream: str, kind: str, trigger_name: str, extra_args: Optional[List[str]] = None) -> Tuple[int, Optional[Dict[str, Any]], List[Dict[str, Any]]]:
	if not is_stream_active(event_stream):
		CevContext.fatal("event stream must exist and be active to call trigger", event_stream=event_stream)

	if not is_valid_event_name(trigger_name):
		CevContext.fatal("invalid trigger name", trigger_name=trigger_name)

	if event_stream == "system-events/extraction":
		if trigger_name == "execute-queue":
			CevContext.fatal("can't execute extraction events yet")
		else:
			CevContext.fatal("unknown scheduled queue trigger name")

	if event_stream == "system-events/scheduled":
		if trigger_name == "execute-queue":
			if execute_scheduled_events(extra_args):
				return 0, None, []
			else:
				return 1, None, []
		else:
			CevContext.fatal("unknown scheduled queue trigger name")

	data_dir = CevContext.data_dir()
	event_stream_dir = data_dir / event_stream

	trigger_path = event_stream_dir / f"{kind}s" / trigger_name
	if not trigger_path.exists():
		CevContext.fatal("specified trigger task does not exist with the specified type", kind=kind)

	env_vars = {
		"CEV_BIN": sys.argv[0],
		"CEV_DATA_DIR": str(data_dir),
		"CEV_EVENT_STREAM": event_stream,
		"CEV_TRIGGER_NAME": trigger_name
	}

	latest_snapshot = get_latest_state_file(event_stream)
	if latest_snapshot:
		env_vars["CEV_LATEST_STATE_SNAPSHOT"] = str(latest_snapshot)

	trig_env = os.environ.copy()
	trig_env.update(env_vars)

	cmd = [str(trigger_path)]
	if extra_args:
		cmd.extend(extra_args)

	CevContext.debug("executing trigger command", cmd=str(cmd), env=str(env_vars))

	try:
		import select
		import fcntl
		import errno

		target_time_limit = 60
		kill_grace_period = 15
		warning_time_limit = target_time_limit - kill_grace_period

		stdout_buffer = ""
		stderr_buffer = ""

		output_commands = []
		start_time = time.time()
		process = subprocess.Popen(cmd, env=trig_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

		# We use non-blocking mode to process log entries live
		for pipe in [process.stdout, process.stderr]:
			flags = fcntl.fcntl(pipe.fileno(), fcntl.F_GETFL)
			fcntl.fcntl(pipe.fileno(), fcntl.F_SETFL, flags | os.O_NONBLOCK)

		warning_sent = False

		while process.poll() is None:
			current_time = time.time()
			elapsed_time = current_time - start_time

			if elapsed_time > warning_time_limit and not warning_sent:
				# Send SIGHUP first as a warning
				CevContext.warn("trigger reached warning limit timeout, sending warning SIGHUP signal")
				process.send_signal(signal.SIGHUP)
				warning_sent = True

			if elapsed_time > target_time_limit:
				CevContext.warn("trigger still running after warning signal and grace period elapsed, killing process")
				process.kill()
				break

			try:
				ready_pipes, _, _ = select.select([process.stdout, process.stderr], [], [], 0.1)
			except select.error:
				# Handle possible interruption in select
				continue

			for pipe in ready_pipes:
				try:
					if pipe == process.stdout:
						chunk = process.stdout.read()
						if chunk:
							stdout_buffer += chunk
					elif pipe == process.stderr:
						chunk = process.stderr.read()
						if chunk:
							stderr_buffer += chunk

				except (IOError, OSError) as e:
					if e.errno != errno.EAGAIN:	# Ignore would-block errors
						raise

		try:
			final_stdout = process.stdout.read()
			if final_stdout:
				stdout_buffer += final_stdout

			final_stderr = process.stderr.read()
			if final_stderr:
				stderr_buffer += final_stderr
				stderr_buffer = process_stderr_buffer(stderr_buffer, output_commands)
		except (IOError, OSError) as e:
			if e.errno != errno.EAGAIN:
				raise

		# Process any remaining stderr content after the process completes
		stderr_buffer = process_stderr_buffer(stderr_buffer, output_commands).strip()
		if stderr_buffer:
			command = filter_trigger_command(stderr_buffer)
			if command:
				output_commands.append(command)

		new_state = None
		stdout_buffer = stdout_buffer.strip()

		if len(stdout_buffer) > 0:
			try:
				new_state = json.loads(stdout_buffer)
				if not isinstance(new_state, dict):
					CevContext.warn("trigger produced non-dict JSON on stdout")
					new_state = None
			except json.JSONDecodeError:
				CevContext.warn("trigger produced invalid output on stdout")
				new_state = None

		exit_status = process.returncode if process.returncode is not None else 200

		return exit_status, new_state, output_commands
	except subprocess.TimeoutExpired:
		CevContext.error("trigger timed out")
		process.kill()
		return 201, None, []
	except Exception as e:
		CevContext.error("unexpected exception running trigger", error=str(e))
		return 237, None, []


def schedule_extraction(watcher_stream: str, target_stream: str, extractor: str, state_name: str) -> bool:
	return cmd_add_event(AddEventArgs(
		event_stream="system-events/extraction",
		event_name="extract-task",
		payload={
			"watcher": watcher_stream,
			"target": target_stream,
			"extractor": extractor,
			"state": state_name
		}
	))


def set_stream_index(event_stream: str, index_kind: str, stream_index: int):
	CevContext.debug("updating stream index", event_stream=event_stream, index_kind=index_kind, stream_index=stream_index)
	index_file = CevContext.data_dir() / event_stream / f"{index_kind}.idx"
	index_file.write_text(f"{stream_index:08x}")


def validate_against_schema(schema: Dict[str, Any], data: Dict[str, Any]) -> bool:
	try:
		jsonschema.validate(data, schema)
		return True
	except jsonschema.exceptions.SchemaError:
		CevContext.error("provided schema wasn't valid")
		return False
	except jsonschema.exceptions.ValidationError:
		return False


def validate_command(command: Dict[str, Any], restricted: bool) -> bool:
	command_identity = command.get("command")
	if not command_identity:
		CevContext.error("command identity missing")
		return False

	with CevContext.context(command=True, command_identity=command_identity, restricted=restricted):
		schema = get_embedded_schema(f"cmd-{command_identity}-command-schema")
		if not schema:
			CevContext.error("provided command not recognized")
			return False

		if restricted and command_identity != "add-event":
			CevContext.error("mutating command found in restricted command environment")
			return False

		try:
			jsonschema.validate(command, schema)
		except jsonschema.exceptions.ValidationError as e:
			CevContext.error("provided command did not validate against expected schema", error=str(e))
			return False

	return True


def validate_command_list(commands: List[Dict[str, Any]], restricted: bool = True) -> bool:
	for cmd in commands:
		if not validate_command(cmd, restricted):
			return False

	return True


def is_valid_event_name(event_name: str) -> bool:
	if not event_name:
		return False

	pattern = r'^([a-z0-9-]{3,64})$'
	return bool(re.match(pattern, event_name))


def is_valid_stream_name(event_stream: str) -> bool:
	if not event_stream:
		return False

	pattern = r"^[a-z0-9-]{3,32}(/[a-z0-9-]{3,32})*$"
	return bool(re.match(pattern, event_stream))


def validate_json_schema(schema: Dict[str, Any]) -> bool:
	try:
		jsonschema.Draft7Validator.check_schema(schema)
		return True
	except jsonschema.exceptions.SchemaError as e:
		return False


def is_stream_active(event_stream: str) -> bool:
	if not is_valid_stream_name(event_stream):
		CevContext.error("invalid event stream name")
		return False

	# Handle the special system event streams
	if event_stream in {"system-events/scheduled", "system-events/extraction"}:
		return True

	event_stream_dir = CevContext.data_dir() / event_stream
	if not event_stream_dir.exists():
		CevContext.error("event stream does not exist")
		return False

	return True


@dataclass
class AddEventArgs:
	event_stream: str
	event_name: str
	payload: Dict[str, Any]

def cmd_add_event(args: AddEventArgs) -> bool:
	if not is_stream_active(args.event_stream):
		CevContext.error("event stream must exist and be active to add an event", event_stream=args.event_stream)
		return False

	if not is_valid_event_name(args.event_name):
		CevContext.error("invalid event name", event_stream=args.event_stream, event_name=args.event_name)
		return False

	with CevContext.context(event_stream=args.event_stream, event_name=args.event_name):
		# Recording any event, event the system ones require a lock in the appropriate
		# directory
		event_stream_dir = CevContext.data_dir() / args.event_stream
		with CevLock(event_stream_dir, "event"):
			schema = get_schema(args.event_stream, trigger_name=args.event_name)
			if schema is None:
				CevContext.error("unknown event can't be added")
				return False

			if not validate_against_schema(schema, args.payload):
				CevContext.error("event failed to validate against schema", schema=schema, event=args.payload)
				return False

			current_index = get_stream_index(args.event_stream, "current")
			if current_index is None:
				new_index = 0
			else:
				new_index = current_index + 1

			recorded_at = datetime.now(timezone.utc).isoformat()

			events_dir = event_stream_dir / "events"
			events_dir.mkdir(parents=True, exist_ok=True)
			event_path = events_dir / f"{new_index:08x}-{recorded_at}-{args.event_name}.evt"

			with open(event_path, "w") as f:
				json.dump(args.payload, f, indent=2)

			set_stream_index(args.event_stream, "current", new_index)

		CevContext.debug("added event", stream_index=new_index)

	return True


def cmd_create_event(event_stream: str, event_name: str, output_path: Optional[Path]) -> bool:
	with CevContext.context(event_stream=event_stream, event_name=event_name):
		if not is_valid_stream_name(event_stream):
			CevContext.error("invalid event stream name")
			return False

		if not is_valid_event_name(event_name):
			CevContext.error("invalid event name")
			return False

		requested_event_schema = get_schema(event_stream, trigger_name=event_name)
		if not requested_event_schema:
			CevContext.error("unable to locate reference schema for provided event")
			return False

		stub_event = generate_stub_from_schema(requested_event_schema)

		if output_path:
			full_output_path = Path(output_path).expanduser().resolve()
			if full_output_path.exists():
				CevContext.error("provided path already exists, cowardly refusing to overwrite file")
				return False

			try:
				with open(full_output_path, "w") as f:
					json.dump(stub_event, f, indent=2)
			except Exception as e:
				CevContext.error("failed to write out event stub", error=str(e))
				return False
		else:
			print(json.dumps(stub_event, indent=2))

	CevContext.debug("created event stub based off existing schema")
	return True


@dataclass
class DefineEventArgs:
	event_stream: str
	event_name: str
	schema: Dict[str, Any]

def cmd_define_event(args: DefineEventArgs) -> bool:
	if not is_valid_stream_name(args.event_stream):
		CevContext.error("invalid event stream name", event_stream=args.event_stream)
		return False

	if not is_valid_event_name(args.event_name):
		CevContext.error("invalid event name", event_name=args.event_name)
		return False

	with CevContext.context(event_stream=args.event_stream, event_name=args.event_name):
		if not validate_json_schema(args.schema):
			CevContext.error("schema for event definition isn't valid")
			return False

		data_dir = CevContext.data_dir()
		with CevLock(data_dir, "config"):

			event_stream_dir = data_dir / args.event_stream
			with CevLock(event_stream_dir, "event"):
				# Read existing active versions we already have them
				active_versions = get_active_schema_versions(args.event_stream)

				current_date_prefix = datetime.now().strftime('%Y%m%d')
				schema_version = f"{current_date_prefix}00"

				schema_dir = event_stream_dir / "schemas"
				schema_dir.mkdir(parents=True, exist_ok=True)

				# Figure out what our new schema version is for this event name
				if args.event_name in active_versions:
					current_version = active_versions[args.event_name]
					version_parts = current_version.split('.')[-1]

					if version_parts.startswith(current_date_prefix):
						version_number = int(version_parts[8:]) + 1
						schema_version = f"{current_date_prefix}{version_number:02d}"

					existing_schema_path = schema_dir / f"{current_version}.json"
					existing_schema = read_json_file(existing_schema_path)
					if not existing_schema:
						CevContext.error("failed to read existing schema")
						return False

					if existing_schema == args.schema:
						CevContext.debug("schema unchanged, no update needed")
						return True

				schema_path = schema_dir / f"{args.event_name}.{schema_version}.json"
				with open(schema_path, "w") as f:
					json.dump(args.schema, f, indent=2)

				active_versions[args.event_name] = f"{args.event_name}.{schema_version}"

				active_version_path = event_stream_dir / "schemas/active_versions"
				with open(active_version_path, "w") as f:
					for version in sorted(active_versions.values()):
						f.write(f"{version}\n")

		CevContext.debug("defined event schema", schema_version=schema_version)
		return True


@dataclass
class DefineTriggerArgs:
	event_stream: str
	trigger_name: str
	trigger_kind: str
	trigger_path: str
	copy_trigger: bool

def cmd_define_trigger(args: DefineTriggerArgs):
	if not is_valid_stream_name(args.event_stream):
		CevContext.error("invalid event stream name", event_stream=args.event_stream)
		return

	if not is_valid_event_name(args.trigger_name):
		CevContext.error("invalid trigger name", trigger_name=args.trigger_name)
		return

	with CevContext.context(event_stream=args.event_stream, trigger_kind=args.trigger_kind, trigger_name=args.trigger_name):
		if not os.path.isfile(args.trigger_path):
			CevContext.fatal("trigger file does not exist", trigger_path=str(args.trigger_path))

		if not os.access(args.trigger_path, os.X_OK):
			CevContext.fatal("trigger file is not executable", trigger_path=str(args.trigger_path))

		data_dir = CevContext.data_dir()
		with CevLock(data_dir, "config"):
			event_stream_dir = data_dir / args.event_stream
			trigger_dir = event_stream_dir / f"{args.trigger_kind}s"

			if args.trigger_kind not in {'handler', 'extractor', 'task'}:
				CevContext.fatal("undefined trigger kind", trigger_kind=args.trigger_kind)

			if args.trigger_kind == "extractor":
				lock_type = "extraction"
			else:
				lock_type = "event"

			with CevLock(event_stream_dir, lock_type):
				trigger_dir.mkdir(parents=True, exist_ok=True)
				new_trigger_path = trigger_dir / args.trigger_name

				if args.copy_trigger:
					with open(args.trigger_path, "rb") as src, open(new_trigger_path, "wb") as trig:
						trig.write(src.read())

					new_trigger_path.chmod(0o755)
				else:
					new_trigger_path.symlink_to(args.trigger_path)

		CevContext.debug("defined trigger")


def cmd_info():
	data_dir = CevContext.data_dir()
	CevContext.info("system info", data_directory=str(data_dir), version=VERSION)


def cmd_is_clean(event_stream: str) -> bool:
	with CevContext.context(event_stream=event_stream):
		if not is_stream_active(event_stream):
			CevContext.fatal("event stream must exist and be active to check dirty status")

		data_dir = CevContext.data_dir()
		event_stream_dir = data_dir / event_stream

		current_index = get_stream_index(event_stream, "current")
		if current_index is None:
			CevContext.debug("stream is clean (no events)", status="clean")
			return True

		processed_index = get_stream_index(event_stream, "processed")
		if processed_index is None:
			CevContext.debug("stream is dirty (no processed events)", status="dirty")
			return False

		with CevContext.context(current_index=current_index, processed_index=processed_index):
			if current_index > processed_index:
				CevContext.debug("stream is dirty (unprocessed events)", status="dirty")
				return False

			CevContext.info("stream is clean", status="clean")
			return True


def cmd_is_newer(event_stream: str, stream_index: int) -> bool:
	with CevContext.context(event_stream=event_stream, stream_index=stream_index):
		if not is_stream_active(event_stream):
			CevContext.error("event stream must exist and be active to check if an index has been included in the state")
			return False

		event_stream_dir = CevContext.data_dir() / event_stream
		processed_index = get_stream_index(event_stream, "processed")

		if processed_index is None:
			CevContext.info("state doesn't exist yet (no processed events)")
			return False

		with CevContext.context(processed_index=processed_index):
			if processed_index > stream_index:
				CevContext.info("state is newer than the provided id")
				return True
			else:
				CevContext.info("state has not been processed beyond the provided id")
				return False


def cmd_process(event_stream: str) -> bool:
	with CevContext.context(event_stream=event_stream):
		CevContext.debug("processing event stream")

		if event_stream == "system-events/extraction":
			return process_extraction_events()

		if event_stream == "system-events/scheduled":
			return process_scheduled_event_stream()

		if not is_stream_active(event_stream):
			CevContext.error("event stream must be active to process oustanding events")
			return False

		event_stream_dir = CevContext.data_dir() / event_stream
		with CevLock(event_stream_dir, "state"):
			current_index = get_stream_index(event_stream, "current")
			if current_index is None:
				CevContext.info("no events to process")
				return True

			processed_index = get_stream_index(event_stream, "processed")
			if processed_index is None:
				next_index = 0
			else:
				next_index = processed_index + 1

			CevContext.debug("processing event range", next_index=next_index, current_index=current_index)

			# Note: if processing the stream results in new events to the stream being processed those
			# will not be handled just queued for future runs. This is by design to ensure event streams
			# have bounded execution.
			for idx in range(next_index, current_index + 1):
				events_dir = event_stream_dir / "events"
				event_files = list(events_dir.glob(f"{idx:08x}-*"))

				if not event_files:
					CevContext.error("expected stream index but found gap in sequence", stream_index=f"{idx:08x}")
					return False

				# Technically there could be multiple events with the same index if there is a concurrency
				# bug, the event lock should handle this directly
				event_path = events_dir / event_files[0]
				_, recorded_at, event_name = meta_from_event_file(event_path)

				with CevContext.context(event_id=idx, event_name=event_name):
					CevContext.debug("running stream handler")
					start_time = time.time()

					# BEGIN PROCESS SPECIFIC HANDLING

					extra_args = [str(event_path)]
					exit_code, state_update, commands = run_trigger(event_stream, "handler", event_name, extra_args)
					duration_ms = int((time.time() - start_time) * 1000)

					if exit_code != 0:
						CevContext.warn("stream handler failed", event_id=idx, duration_ms=duration_ms, exit_code=exit_code)
						return False

					if validate_command_list(commands, False):
						execute_commands(commands)
					else:
						CevContext.warn("stream handler produced invalid commands", duration_ms=duration_ms, exit_code=exit_code)
						return False

					# END PROCESS SPECIFIC HANDLING

					if state_update:
						persist_state(event_stream, idx, state_update)

					set_stream_index(event_stream, "processed", idx)
					CevContext.debug("stream handler succeeded", event_id=idx, duration_ms=duration_ms, exit_code=exit_code)

	return True


def cmd_run(event_stream: str, trigger_name: str, extra_args: Optional[List[str]] = None) -> Tuple[bool, Optional[Dict[str, Any]]]:
	if extra_args and extra_args[0] == "--":
		extra_args = extra_args[1:]

	with CevContext.context(event_stream=event_stream, trigger_name=trigger_name, args=extra_args):
		CevContext.debug("running trigger")

		if not cmd_is_clean(event_stream):
			CevContext.debug("processing dirty stream before running trigger")
			if not cmd_process(event_stream):
				CevContext.error("failed to process stream not executing trigger")
				return False, None

		start_time = time.time()
		exit_code, state_update, commands = run_trigger(event_stream, "task", trigger_name, extra_args)
		duration_ms = int((time.time() - start_time) * 1000)

		# Only perform additional processing if the command was successful
		if exit_code != 0:
			CevContext.error("trigger exited with error", duration_ms=duration_ms, exit_code=exit_code)
			return False, None

		# Commands are how tasks can modify state in this stream and others, we want to have high
		# assurity that they're all valid before we attempt to apply them to our state.
		if validate_command_list(commands, False):
			execute_commands(commands)
		else:
			CevContext.error("trigger produced invalid command list", duration_ms=duration_ms, exit_code=exit_code)
			return False, None

		CevContext.debug("trigger completed successfully", duration_ms=duration_ms, exit_code=exit_code)

	return True, state_update


#@dataclass
#class UnwatchArgs:
#  watcher_stream: str
#  target_stream: str

#def cmd_unwatch(args: UnwatchArgs, logger: CevLogger, data_dir: Path):
#  if not is_valid_stream_name(args.watcher_stream):
#    logger.fatal("invalid watcher stream name", watcher_stream=args.watcher_stream)
#
#  if not is_valid_stream_name(args.target_stream):
#    logger.fatal("invalid target stream name", target_stream=args.target_stream)
#
#  # Check if streams exist
#  watcher_stream_dir = get_event_stream_directory(data_dir, args.watcher_stream)
#  target_stream_dir = get_event_stream_directory(data_dir, args.target_stream)
#
#  if not watcher_stream_dir.exists():
#    logger.fatal("watcher stream does not exist", watcher_stream=args.watcher_stream)
#
#  if not target_stream_dir.exists():
#    logger.fatal("target stream does not exist", target_stream=args.target_stream)
#
#  # Update watchers file in target stream
#  watchers_file = target_stream_dir / "watchers"
#  if not watchers_file.exists():
#    logger.info("no watchers defined", target_stream=args.target_stream)
#    return
#
#  # Remove the watcher from the file
#  watchers = []
#  removed = False
#  with open(watchers_file, "r") as f:
#    for line in f:
#      entry = line.strip()
#      if entry.split(":")[0] != args.watcher_stream:
#        watchers.append(entry)
#      else:
#        removed = True
#
#  # Write back the updated watchers list
#  with open(watchers_file, "w") as f:
#    for entry in watchers:
#      f.write(f"{entry}\n")
#
#  # Remove entry from watched_streams file in watcher stream if it exists
#  watched_streams_file = watcher_stream_dir / "watched_streams"
#  if watched_streams_file.exists():
#    watched_streams = []
#      with open(watched_streams_file, "r") as f:
#        for line in f:
#          entry = line.strip()
#          if not entry.startswith(f"{args.target_stream}:"):
#            watched_streams.append(entry)
#
#      with open(watched_streams_file, "w") as f:
#        for entry in watched_streams:
#          f.write(f"{entry}\n")


@dataclass
class WatchArgs:
	watcher: str
	target: str
	extractor: str

def cmd_watch(args: WatchArgs) -> bool:
	if not is_stream_active(args.watcher):
		CevContext.error("watching event stream must exist and be active")
		return False

	if not is_stream_active(args.target):
		CevContext.error("target event stream must exist and be active")
		return False

	if not is_valid_event_name(args.extractor):
		CevContext.error("invalid extration trigger name", extractor=args.extractor)
		return False

	with CevContext.context(watcher=args.watcher, target=args.target, extractor=args.extractor):
		target_stream_dir = CevContext.data_dir() / args.target
		with CevLock(target_stream_dir, "state"):
			watchers = set()
			watchers_file = target_stream_dir / "watchers"

			if watchers_file.exists():
				with open(watchers_file, "r") as f:
					for line in f:
						watchers.add(line.strip())

			watchers.add(f"{args.watcher}:{args.extractor}")

			with open(watchers_file, "w") as f:
				for entry in watchers:
					f.write(f"{entry}\n")

		# We intentionally do not process the target event stream, if it hasn't already been processed or
		# there are no events to process we produce a warning but this isn't a failure. When the state
		# eventually does get produced by the target stream it will automatically enqueue one of these
		# events.
		latest_snapshot = get_latest_state_file(args.target)
		if latest_snapshot:
			schedule_extraction(args.watcher, args.target, args.extractor, latest_snapshot.name)
		else:
			CevContext.warn("no state for target stream, not automatically including an extraction event")

		return True


def main():
	parser = argparse.ArgumentParser(description="Common Event system")

	# Global arguments
	parser.add_argument("-v", "--verbose", action="store_true", help="output debug information")
	parser.add_argument("-d", "--data-dir", help="Override default data directory")

	subparsers = parser.add_subparsers(dest="command", required=True)

	# Command specific arguments
	add_event_parser = subparsers.add_parser("add-event", help="Add an event to a stream")
	add_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	add_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	add_event_parser.add_argument("-f", "--file", help="Event JSON file path")
	add_event_parser.add_argument("-p", "--process", action="store_true", help="Process stream after adding event")

	create_event_parser = subparsers.add_parser("create-event", help="Create an event stub")
	create_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	create_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	create_event_parser.add_argument("-f", "--file", help="Output file path")

	define_event_parser = subparsers.add_parser("define-event", help="Define an event schema")
	define_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	define_event_parser.add_argument("-n", "--name", required=True, help="Event name")
	define_event_parser.add_argument("-f", "--file", help="JSON schema file path")

	define_trigger_parser = subparsers.add_parser("define-trigger", help="Define a trigger")
	define_trigger_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	define_trigger_parser.add_argument("-n", "--name", required=True, help="Trigger name")
	define_trigger_parser.add_argument("-t", "--trigger", required=True, help="Trigger executable path")
	define_trigger_parser.add_argument("-k", "--kind", required=True, help="Trigger kind (handler, extractor, task)")
	define_trigger_parser.add_argument("-c", "--copy", action="store_true", help="Copy the provided executable into the data directory (will symlink if not copied)")

	is_clean_parser = subparsers.add_parser("is-clean", help="Verify all events are in a stream have been processed")
	is_clean_parser.add_argument("-s", "--stream", required=True, help="Event stream name")

	is_newer_parser = subparsers.add_parser("is-newer", help="Check if a stream's latest state snapshot is newer than provided index")
	is_newer_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	is_newer_parser.add_argument("-i", "--index", required=True, help="Stream index to compare against (in base16)")

	subparsers.add_parser("info", help="Display system summary information")

	process_parser = subparsers.add_parser("process", help="Process events in a stream")
	process_parser.add_argument("-s", "--stream", required=True, help="Event stream name")

	run_parser = subparsers.add_parser("run", help="Run a specific task in an event stream")
	run_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
	run_parser.add_argument("-n", "--name", required=True, help="Task trigger name")
	run_parser.add_argument("extra_args", nargs=argparse.REMAINDER, help="Additional arguments to pass to the trigger (after --)")

	#unwatch_parser = subparsers.add_parser("unwatch", help="Remove a watched stream relationship")
	#unwatch_parser.add_argument("-w", "--watcher", required=True, help="Watcher stream name")
	#unwatch_parser.add_argument("-t", "--target", required=True, help="Target stream to watch")
	#unwatch_parser.add_argument("-e", "--extractor", required=True, help="Name of state extraction trigger")

	watch_parser = subparsers.add_parser("watch", help="Establish a watched stream relationship")
	watch_parser.add_argument("-w", "--watcher", required=True, help="Watcher stream name")
	watch_parser.add_argument("-t", "--target", required=True, help="Target stream to watch")
	watch_parser.add_argument("-e", "--extractor", required=True, help="Name of state extraction trigger")

	args = parser.parse_args()
	data_dir = get_data_directory(args.data_dir)

	CevContext.init(data_dir, args.verbose)
	CevContext.debug("starting new session", command=args.command, version=VERSION)

	if args.command == "add-event":
		event_path = Path(args.file).expanduser().resolve()
		if not event_path.exists():
			CevContext.fatal("provided file doesn't exist", event_path=event_path)

		payload = event_path.read_text().strip()
		cmd_add_event(AddEventArgs(
			event_stream=args.stream,
			event_name=args.name,
			payload=payload,
		))

		# Process event immediately if requested by the caller
		if args.process:
			cmd_process(args.stream)
	elif args.command == "create-event":
		cmd_create_event(args.stream, args.name, args.file)
	elif args.command == "define-event":
		schema_path = Path(args.file).expanduser().resolve()

		schema = read_json_file(schema_path)
		if not schema:
			CevContext.error("unable to access provided schema", schema_path=schema_path)

		cmd_define_event(DefineEventArgs(
			event_stream=args.stream,
			event_name=args.name,
			schema=schema,
		))
	elif args.command == "define-trigger":
		trigger_path = Path(args.trigger).expanduser().resolve()

		cmd_define_trigger(DefineTriggerArgs(
			event_stream=args.stream,
			trigger_name=args.name,
			trigger_kind=args.kind,
			trigger_path=trigger_path,
			copy_trigger=args.copy,
		))
	elif args.command == "info":
		cmd_info()
	elif args.command == "is-clean":
		if cmd_is_clean(args.stream):
			sys.exit(0)
		else:
			sys.exit(1)
	elif args.command == "is-newer":
		try:
			stream_index = int(args.index, 16)
		except ValueError:
			# This one however is a user error and may not be the correct response but its the best we can do
			CevContext.fatal("invalid stream index format (should be base16)")

		if cmd_is_newer(args.stream, stream_index):
			sys.exit(0)
		else:
			sys.exit(1)
	elif args.command == "process":
		cmd_process(args.stream)
	elif args.command == "run":
		cmd_run(args.stream, args.name, args.extra_args)
	elif args.command == "watch":
		cmd_watch(WatchArgs(
			watcher=args.watcher,
			target=args.target,
			extractor=args.extractor,
		))
	#elif args.command == "unwatch":
	#  unwatch_args = UnwatchArgs(
	#    watcher_stream=args.watcher,
	#    target_stream=args.target,
	#  )
	#  cmd_unwatch(unwatch_args, logger, data_dir)

if __name__ != "__main__":
	print("utility is not available as a library", file=sys.stderr)
	sys.exit(1)

# Explicit exit is important to ensure we don't try and execute the embedded schemas
sys.exit(main())

__EMBEDDED_SCHEMAS__
# [system-events-extraction-extract-task-schema]
{
	"type": "object",
	"properties": {
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"},
		"state": {"type": "string"}
	},
	"required": ["watcher", "target", "extractor", "state"]
}

# [system-events-scheduled-complete-task-schema]
{
	"type": "object",
	"properties": {
		"task_id": {"type": "string"},
		"ctx_id": {"type": "string"},
		"status": {"type": "string", "enum": ["success", "failure"]}
	},
	"required": ["task_id", "ctx_id", "status"]
}

# [system-events-scheduled-state-schema]
{
	"type": "object",
	"properties": {
		"scheduled_tasks": {
			"type": "array",
			"items": {
				"type": "object",
				"properties": {
					"task_id": {"type": "string"},
					"event_stream": {"type": "string"},
					"trigger_name": {"type": "string"},
					"extra_args": { "type": "array", "items": { "type": "string" } },
					"scheduled_for": {"type": "string", "format": "date-time"}
				},
				"required": ["task_id", "event_stream", "trigger_name", "extra_args", "scheduled_for"]
			}
		}
	},
	"required": ["scheduled_tasks"]
}

# [system-events-scheduled-schedule-task-schema]
{
	"type": "object",
	"properties": {
		"event_stream": {"type": "string"},
		"trigger_name": {"type": "string"},
		"extra_args": { "type": "array", "items": { "type": "string" } },
		"scheduled_for": {"type": "string", "format": "date-time"}
	},
	"required": ["event_stream", "trigger_name", "scheduled_for"]
}

# [cmd-add-event-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["add-event"]},
		"event_stream": {"type": "string"},
		"event_name": {"type": "string"},
		"payload": {"type": "object"}
	},
	"required": ["command", "event_stream", "event_name", "payload"]
}

# [cmd-define-event-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["define-event"]},
		"event_stream": {"type": "string"},
		"event_name": {"type": "string"},
		"schema": {"type": "object"}
	},
	"required": ["command", "event_stream", "event_name", "schema"]
}

# [cmd-define-trigger-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["define-trigger"]},
		"event_stream": {"type": "string"},
		"trigger_name": {"type": "string"},
		"trigger_kind": {"type": "string"},
		"trigger_path": {"type": "string"},
		"copy_trigger": {"type": "bool"}
	},
	"required": ["command", "event_stream", "trigger_name", "trigger_kind", "trigger_path"]
}

# [cmd-unwatch-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["unwatch"]},
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"}
	},
	"required": ["command", "watching_stream", "event_stream"]
}

# [cmd-watch-command-schema]
{
	"type": "object",
	"properties": {
		"command": {"type": "string", "enum": ["watch"]},
		"watcher": {"type": "string"},
		"target": {"type": "string"},
		"extractor": {"type": "string"}
	},
	"required": ["command", "watching_stream", "event_stream", "extractor"]
}

#!/usr/bin/env -S uv run -q -s

# /// script
# requires-python = ">=3.12"
# dependencies = ["jsonschema>=4.23"]
# ///

import argparse
import fcntl
import json
import os
import signal
import subprocess
import sys
import time

from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import jsonschema

VERSION = "0.3.0"

class CevLock:
    def __init__(self, lock_file: Path, lock_type: str = "state"):
        self.lock_file = lock_file

        # Set timeout based on lock type according to specification, using seconds
        if lock_type == "write":
            self.timeout_seconds = 5
        elif lock_type == "delayed":
            self.timeout_seconds = 2  # Short-lived lock for delayed file
        else:  # state lock
            self.timeout_seconds = 15 * 60  # 15 minutes in seconds

        self.lock_fd = None
        self.lock_type = lock_type

    def __enter__(self):
        self.lock_file.parent.mkdir(parents=True, exist_ok=True)
        self.lock_fd = open(self.lock_file, "w+")

        try:
            fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            start_time = time.time()

            while time.time() - start_time < self.timeout_seconds:
                try:
                    fcntl.flock(self.lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                    break
                except BlockingIOError:
                    time.sleep(0.1)
            else:
                raise RuntimeError(f"could not acquire {self.lock_type} lock on {self.lock_file} within {self.timeout_seconds} seconds")

        self.lock_fd.write(str(os.getpid()))
        self.lock_fd.flush()

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.lock_fd:
            fcntl.flock(self.lock_fd, fcntl.LOCK_UN)

            self.lock_fd.close()
            self.lock_fd = None


class LogLevel(str, Enum):
    DEBUG = "debug"
    INFO = "info"
    WARN = "warn"
    ERROR = "error"
    FATAL = "fatal"


class CevLogger:
    def __init__(self, log_file: Path, verbose: bool = False):
        self.log_file = log_file
        self.verbose = verbose
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

    def log(self, level: LogLevel, message: str, **kwargs):
        timestamp = datetime.now(timezone.utc).isoformat()
        log_entry = {
            "timestamp": timestamp,
            "level": level.value,
            "message": message,
            **kwargs
        }

        with open(self.log_file, "a") as f:
            f.write(json.dumps(log_entry) + "\n")

        if self.verbose or level != LogLevel.DEBUG:
            system_output = json.dumps(log_entry)
            sys.stderr.write(f"{system_output}\n")

    def debug(self, message: str, **kwargs):
        self.log(LogLevel.DEBUG, message, **kwargs)

    def info(self, message: str, **kwargs):
        self.log(LogLevel.INFO, message, **kwargs)

    def warn(self, message: str, **kwargs):
        self.log(LogLevel.WARN, message, **kwargs)

    def error(self, message: str, **kwargs):
        self.log(LogLevel.ERROR, message, **kwargs)

    def fatal(self, message: str, **kwargs):
        self.log(LogLevel.FATAL, message, **kwargs)
        sys.exit(1)


def create_state_snapshot(event_stream_dir: Path, data_dir: Path, event_stream: str,
                         event_id: str, state_data: Dict[str, Any],
                         processing_duration_ms: Optional[int] = None) -> Path:
    state_dir = event_stream_dir / "state"
    state_dir.mkdir(exist_ok=True)

    timestamp = datetime.now(timezone.utc).isoformat()

    previous_snapshot = get_latest_state_snapshot(event_stream_dir)
    previous_snapshot_name = previous_snapshot.name.replace(".state.json", "") if previous_snapshot else ""

    dependencies = get_dependency_info(event_stream_dir, data_dir)

    metadata = {
        "stream": event_stream,
        "previous_snapshot": previous_snapshot_name,
        "event_id": event_id,
        "timestamp": timestamp,
        "dependencies": dependencies
    }

    if processing_duration_ms is not None:
        metadata["processing_duration_ms"] = processing_duration_ms

    snapshot = {
        "metadata": metadata,
        "state": state_data
    }

    state_path = get_state_path(event_stream_dir, event_id, timestamp)
    with open(state_path, "w") as f:
        json.dump(snapshot, f, indent=2)

    return state_path


def generate_stub_from_schema(schema: Dict[str, Any]) -> Dict[str, Any]:
    if "type" not in schema:
        return {}

    if schema["type"] == "object":
        result = {}

        for prop_name, prop_schema in schema.get("properties", {}).items():
            result[prop_name] = generate_stub_from_schema(prop_schema)

        return result
    elif schema["type"] == "array":
        if "items" in schema:
            return [generate_stub_from_schema(schema["items"])]

        return []
    elif schema["type"] == "string":
        if schema.get("format") == "date-time":
            return datetime.now(timezone.utc).isoformat()

        return ""
    elif schema["type"] == "boolean":
        return False
    elif schema["type"] == "number" or schema["type"] == "integer":
        return 0
    elif schema["type"] == "null":
        return None

    return None


def get_current_stream_index(event_stream_dir: Path) -> Optional[str]:
    current_index_file = event_stream_dir / "current.idx"

    if not current_index_file.exists():
        return None

    return current_index_file.read_text().strip()


def get_data_directory(args) -> Path:
    if args.data_dir:
        return Path(args.data_dir).expanduser().resolve()

    xdg_data_home = os.environ.get("XDG_DATA_HOME")
    if xdg_data_home:
        return Path(xdg_data_home) / "cev"

    return Path.home() / ".local" / "share" / "cev"


def get_dependency_info(event_stream_dir: Path, data_dir: Path) -> Dict[str, Dict[str, str]]:
    dependency_dir = event_stream_dir / "dependencies"
    dependency_info = {}

    if not dependency_dir.exists():
        return dependency_info

    # Process all symlinks in the dependencies directory
    for dependency_link in dependency_dir.glob("*"):
        if not dependency_link.is_symlink():
            continue

        dependency_stream = dependency_link.name
        dependency_dir_path = data_dir / dependency_stream

        # Skip if dependency stream doesn't exist
        if not dependency_dir_path.exists():
            continue

        latest_event_id = get_processed_stream_index(dependency_dir_path)
        if latest_event_id is None:
            latest_event_id = get_current_stream_index(dependency_dir_path)

            # Skip dependencies with no events
            if latest_event_id is None:
                continue

        latest_snapshot = get_latest_state_snapshot(dependency_dir_path)

        # Create dependency info with event_id and snapshot (when available)
        dependency_entry = {"event_id": latest_event_id}
        if latest_snapshot:
            snapshot_name = latest_snapshot.name.replace(".state.json", "")
            dependency_entry["snapshot"] = snapshot_name

        dependency_info[dependency_stream] = dependency_entry

    return dependency_info


def get_event_path(event_stream_dir: Path, stream_index: str, timestamp: str, event_name: str) -> Path:
    return event_stream_dir / "events" / f"{stream_index}-{timestamp}-{event_name}.evt"


def get_event_stream_directory(data_dir: Path, event_stream: str) -> Path:
    return data_dir / event_stream


def get_latest_state_snapshot(event_stream_dir: Path) -> Optional[Path]:
    state_dir = event_stream_dir / "state"
    if not state_dir.exists():
        return None

    snapshots = sorted(state_dir.glob("*.state.json"), key=lambda x: str(x))
    if not snapshots:
        return None

    return snapshots[-1]


def get_processed_stream_index(event_stream_dir: Path) -> Optional[str]:
    processed_index_file = event_stream_dir / "processed.idx"

    if not processed_index_file.exists():
        return None

    return processed_index_file.read_text().strip()


def get_state_path(event_stream_dir: Path, stream_index: str, timestamp: str) -> Path:
    return event_stream_dir / "state" / f"{stream_index}-{timestamp}.state.json"


def increment_stream_index(current_index: Optional[str] = None) -> str:
    if current_index is None:
        return "00000000"

    return f"{(int(current_index, 16) + 1):08x}"


def parse_state_snapshot(state_path: Path) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    if not state_path.exists():
        return {}, {}

    try:
        with open(state_path, "r") as f:
            snapshot = json.load(f)

        metadata = snapshot.get("metadata", {})
        state = snapshot.get("state", {})

        return metadata, state
    except (json.JSONDecodeError, FileNotFoundError):
        return {}, {}


def process_trigger_log(log_data: Dict[str, Any], stream: str, trigger_name: str) -> Dict[str, Any]:
    processed_log = log_data.copy()

    # Set level if not present or invalid
    if "level" not in processed_log or processed_log["level"] not in [l.value for l in LogLevel]:
        processed_log["level"] = "info" if "level" not in processed_log else "debug"

    # We always override timestamp, the parent process acts as the authority for this information
    processed_log["timestamp"] = datetime.now(timezone.utc).isoformat()

    # We always want this present, but we want the most specific stream that is triggering this so
    # the deepest leaf node in the execution tree should be the one to set this.
    if "stream" not in processed_log:
        processed_log["stream"] = stream

    # Same for the trigger name
    if "trigger_name" not in processed_log:
        processed_log["trigger_name"] = trigger_name

    return processed_log


def set_current_stream_index(event_stream_dir: Path, stream_index: str):
    current_index_file = event_stream_dir / "current.idx"
    current_index_file.write_text(stream_index)


def set_processed_stream_index(event_stream_dir: Path, stream_index: str):
    processed_index_file = event_stream_dir / "processed.idx"
    processed_index_file.write_text(stream_index)


def validate_event_against_schema(event: Dict[str, Any], schema: Dict[str, Any]) -> Tuple[bool, Optional[str]]:
    try:
        jsonschema.validate(event, schema)
        return True, None
    except jsonschema.exceptions.ValidationError as e:
        return False, str(e)


def validate_event_name(event_name: str) -> bool:
    if not event_name:
        return False

    # Must begin with lowercase letter
    if not event_name[0].islower():
        return False

    prev_char = None
    has_digits = False

    # Check each character
    for i, c in enumerate(event_name):
        if c.islower():
            if has_digits:  # Digits must be at the end
                return False

            prev_char = c
        elif c == "-":
            if has_digits:  # Digits must be at the end
                return False

            if prev_char == "-":  # No repeating hyphens
                return False

            prev_char = c
        elif c.isdigit():
            has_digits = True
        else:
            return False

    return True


def validate_event_stream(event_stream: str) -> bool:
    if not event_stream:
        return False

    parts = event_stream.split("/")
    for part in parts:
        # Each component must be 3-32 chars long
        if len(part) < 3 or len(part) > 32:
            return False

        # Must begin and end with a lowercase letter
        if not part[0].islower() or not part[-1].islower():
            return False

        # Check for lowercase letters and non-repeating hyphens
        prev_char = None
        for c in part:
            if c.islower():
                prev_char = c
            elif c == "-":
                if prev_char == "-":
                    return False

                prev_char = c
            else:
                return False

    return True


def validate_json_schema(schema: Dict[str, Any]) -> bool:
    try:
        jsonschema.Draft7Validator.check_schema(schema)
        return True
    except jsonschema.exceptions.SchemaError as e:
        return False


def run_trigger(trigger_path: Path, event_path: Optional[Path], state_path: Optional[Path],
                env_vars: Dict[str, str], logger: CevLogger, with_timeout: bool = True) -> Tuple[int, Optional[Dict[str, Any]]]:
    process_env = os.environ.copy()
    process_env.update(env_vars)

    cmd = [str(trigger_path)]
    if event_path:
        cmd.append(str(event_path))
    if state_path:
        cmd.append(str(state_path))

    logger.debug("executing trigger command", cmd=str(cmd), env=str(env_vars))

    try:
        process = subprocess.Popen(
            cmd,
            env=process_env,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        target_time_limit = 60
        kill_grace_period = 15
        warning_time_limit = target_time_limit - kill_grace_period

        stdout_data = []
        stderr_data = []

        # Non-blocking read setup
        import select
        import fcntl

        # Make stdout and stderr non-blocking
        for pipe in [process.stdout, process.stderr]:
            flags = fcntl.fcntl(pipe.fileno(), fcntl.F_GETFL)
            fcntl.fcntl(pipe.fileno(), fcntl.F_SETFL, flags | os.O_NONBLOCK)

        start_time = time.time()

        while process.poll() is None:
            # If we have a timeout, we'll need to do some additional signal handling to be gentle
            # to the called triggers
            if with_timeout and time.time() - start_time > warning_time_limit:
                # Send SIGHUP first as a warning
                logger.warn("trigger reached warning limit timeout, sending warning SIGHUP signal",
                            trigger=Path(trigger_path).name)
                process.send_signal(signal.SIGHUP)

                # We'll give the process an extra 15 seconds to try and clean up or finish up
                grace_time = time.time()
                while process.poll() is None and time.time() - grace_time < kill_grace_period:
                    time.sleep(0.1)

                # I gave you a warning... You dare be insolent?
                if process.poll() is None:
                    logger.warn("trigger still running after warning signal, killing process",
                                trigger=Path(trigger_path).name)
                    process.kill()
                    break

            # Check if the process has produced any output
            ready_pipes, _, _ = select.select([process.stdout, process.stderr], [], [], 0.1)

            for pipe in ready_pipes:
                if pipe == process.stdout:
                    data = process.stdout.read()
                    if data:
                        stdout_data.append(data)
                elif pipe == process.stderr:
                    data = process.stderr.read()
                    if data:
                        stderr_data.append(data)

            # todo: if we receive a complete line on stderr we should print it out immediately

        # todo: is it possible there is extra date left on stdout and stderr?

        # Simple output collection, just join our pieces together, could do better here...
        stdout = "".join(stdout_data)
        stderr = "".join(stderr_data)

        # STDERR is the structured logs
        if stderr.strip():
            trigger_name = Path(trigger_path).name
            stream = env_vars.get("CEV_STREAM", "unknown")

            # Each line SHOULD be a valid JSON object but we'll handle and warn for other output
            for line in stderr.strip().split("\n"):
                if line:
                    try:
                        log_entry = json.loads(line)

                        if isinstance(log_entry, dict):
                            processed_log = process_trigger_log(log_entry, stream, trigger_name)
                            log_level = processed_log.pop("level", "info")

                            # We want to pull the message out of the inner log line, this
                            # extraction and rebuild is important to ensure we have proper
                            # precedence and overrides over different variables.
                            message = processed_log.pop("message")

                            if log_level == "debug":
                                logger.debug(message, **processed_log)
                            elif log_level == "info":
                                logger.info(message, **processed_log)
                            elif log_level == "warn":
                                logger.warn(message, **processed_log)
                            elif log_level == "error":
                                logger.error(message, **processed_log)
                            elif log_level == "fatal":
                                logger.fatal(message, **processed_log)

                            continue
                    except json.JSONDecodeError:
                        pass

                # If we got here, it's not a valid JSON log, so log it as stderr output
                logger.warn("raw trigger stderr output", stderr=line, trigger=trigger_name)

        # Check if STDOUT contains a valid state update
        state_update = None
        if stdout.strip():
            try:
                state_update = json.loads(stdout.strip())

                if not isinstance(state_update, dict):
                    logger.warn("trigger produced non-dict JSON on stdout",
                             stream=env_vars.get("CEV_STREAM", "unknown"),
                             trigger_name=env_vars.get("CEV_TRIGGER_NAME", "unknown"))

                    state_update = None
            except json.JSONDecodeError:
                logger.warn("trigger produced invalid output on stdout",
                          stream=env_vars.get("CEV_STREAM", "unknown"),
                          trigger_name=env_vars.get("CEV_TRIGGER_NAME", "unknown"),
                          stdout=stdout.strip())

                state_update = None

        return process.returncode if process.returncode is not None else 1, state_update
    except subprocess.TimeoutExpired:
        trigger_name = Path(trigger_path).name
        logger.error("trigger timed out after 60 seconds", trigger=trigger_name)

        process.kill()

        return 1, None
    except Exception as e:
        trigger_name = Path(trigger_path).name
        logger.error("error running trigger", error=str(e), trigger=trigger_name)
        return 1, None


def verify_dependency_state(event_stream_dir: Path, data_dir: Path, logger: CevLogger) -> bool:
    """Verify that all dependencies are in the expected state.
    Returns True if all dependencies are valid, False if any are invalid."""

    dependency_dir = event_stream_dir / "dependencies"
    if not dependency_dir.exists():
        return True

    # Process all symlinks in the dependencies directory
    for dependency_link in dependency_dir.glob("*"):
        if not dependency_link.is_symlink():
            continue

        dependency_stream = dependency_link.name
        dependency_dir_path = data_dir / dependency_stream

        # Skip if dependency stream doesn't exist
        if not dependency_dir_path.exists():
            logger.error("dependency stream does not exist",
                       dependency=dependency_stream)
            continue

        # Check if there are events in the dependency stream that haven't been processed
        # by this stream
        processed_index = get_processed_stream_index(dependency_dir_path)
        if processed_index is None:
            # No events processed in dependency, so nothing to check
            continue

        # Get the current stream index if available
        latest_event_id = get_current_stream_index(dependency_dir_path)
        if latest_event_id is None:
            latest_event_id = processed_index

        # Get the latest state snapshot of the current stream
        latest_snapshot = get_latest_state_snapshot(event_stream_dir)
        if latest_snapshot is None:
            # This stream has no state yet, so it needs to fetch the dependency state
            logger.info("stream needs to update dependency state",
                      event_stream=str(event_stream_dir.relative_to(data_dir)),
                      dependency=dependency_stream)
            return False

        # Parse metadata from the snapshot to check dependency versions
        metadata, _ = parse_state_snapshot(latest_snapshot)
        dependencies = metadata.get("dependencies", {})

        # Check if the dependency version recorded in our state is older than what's available
        dep_info = dependencies.get(dependency_stream, {})
        recorded_event_id = dep_info.get("event_id", "00000000")

        if int(recorded_event_id, 16) < int(latest_event_id, 16):
            # The dependency has newer events we haven't yet incorporated
            logger.info("dependency has newer events",
                      event_stream=str(event_stream_dir.relative_to(data_dir)),
                      dependency=dependency_stream,
                      recorded=recorded_event_id,
                      current=latest_event_id)
            return False

    return True

def add_stream_to_delayed(event_stream: str, event_stream_dir: Path, logger: CevLogger):
    """Add a stream to the delayed file with proper locking to avoid concurrency issues."""
    delayed_path = event_stream_dir / "delayed"
    delay_lock_path = event_stream_dir / "delayed.flock"

    # Use a short-lived lock for delayed file operations
    with CevLock(delay_lock_path, "delayed"):
        # Read existing delayed streams
        delayed_streams = set()
        if delayed_path.exists():
            with open(delayed_path, "r") as f:
                for line in f:
                    delayed_streams.add(line.strip())

        # Add the current stream if not already present
        if event_stream not in delayed_streams:
            delayed_streams.add(event_stream)

            # Write back the updated list
            with open(delayed_path, "w") as f:
                for stream in delayed_streams:
                    f.write(f"{stream}\n")

            logger.debug("added stream to delayed file", event_stream=event_stream)

def get_embedded_data(key: str) -> str:
    """Retrieve embedded data by key from the trailing data section.
    Returns the raw text content without any parsing."""
    # Get the path to the current script
    script_path = Path(sys.argv[0])

    # Open and read the script file
    with open(script_path, "r") as f:
        script_content = f.read()

    # Split content at the marker
    if "# BEGIN_SCRIPTS" not in script_content:
        return None

    data_section = script_content.split("# BEGIN_SCRIPTS", 1)[1]

    # Parse the data section to find the requested key
    current_key = None
    content_lines = []

    for line in data_section.split("\n"):
        if line.startswith("# [") and line.endswith("]"):
            # If we found our key and are now at a new key, return the content
            if current_key == key and content_lines:
                return "".join(content_lines)

            # Start of a new section
            current_key = line[3:-1]  # Remove "# [" and "]"
            content_lines = []
        elif current_key == key:
            # Collect lines for the current key
            content_lines.append(line + "\n")

    # Check if the key is at the end of the file
    if current_key == key and content_lines:
        return "".join(content_lines)

    return None

def define_event(args, logger, data_dir):
    event_stream = args.stream
    event_name = args.name
    schema_file = args.file

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_name(event_name):
        logger.fatal("invalid event name", event_name=event_name)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    schema_dir = event_stream_dir / "schemas"
    events_dir = event_stream_dir / "events"

    schema_dir.mkdir(parents=True, exist_ok=True)
    events_dir.mkdir(parents=True, exist_ok=True)

    schema_path = schema_dir / f"{event_name}.schema"

    # Get schema from file or stdin
    if schema_file:
        try:
            with open(schema_file, "r") as f:
                schema = json.load(f)
        except json.JSONDecodeError as e:
            logger.fatal("failed to load schema: invalid JSON", error=str(e), file=schema_file)
        except FileNotFoundError as e:
            logger.fatal("failed to load schema: file not found", file=schema_file)
    else:
        try:
            schema = json.load(sys.stdin)
        except json.JSONDecodeError as e:
            logger.fatal("failed to parse schema from stdin", error=str(e))

    if not validate_json_schema(schema):
        logger.fatal("invalid json schema")

    with open(schema_path, "w") as f:
        json.dump(schema, f, indent=2)

    logger.info("defined event schema", event_stream=event_stream, event_name=event_name)

def create_event(args, logger, data_dir):
    event_stream = args.stream
    event_name = args.name
    event_file = args.file

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_name(event_name):
        logger.fatal("invalid event name", event_name=event_name)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    schema_dir = event_stream_dir / "schemas"

    if not schema_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    schema_path = schema_dir / f"{event_name}.schema"

    if not schema_path.exists():
        logger.fatal("schema does not exist for event", event_name=event_name, event_stream=event_stream)

    try:
        with open(schema_path, "r") as f:
            schema = json.load(f)
    except json.JSONDecodeError as e:
        logger.fatal("failed to load schema: invalid JSON", error=str(e), file=str(schema_path))
    except FileNotFoundError:
        logger.fatal("failed to load schema: file not found", file=str(schema_path))

    # Generate stub from schema
    stub = generate_stub_from_schema(schema)

    if event_file:
        with open(event_file, "w") as f:
            json.dump(stub, f, indent=2)
    else:
        logger.info("event stub", event=json.dumps(stub, indent=2))

    logger.debug("created event stub", event_stream=event_stream, event_name=event_name)

def add_event(args, logger, data_dir):
    event_stream = args.stream
    event_name = args.name
    event_file = args.file
    at_timestamp = args.at
    process = args.process

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_name(event_name):
        logger.fatal("invalid event name", event_name=event_name)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    schema_dir = event_stream_dir / "schemas"

    if not schema_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    schema_path = schema_dir / f"{event_name}.schema"

    if not schema_path.exists():
        logger.fatal("schema does not exist for event", event_name=event_name, event_stream=event_stream)

    try:
        with open(schema_path, "r") as f:
            schema = json.load(f)
    except json.JSONDecodeError as e:
        logger.fatal("failed to load schema: invalid JSON", error=str(e), file=str(schema_path))
    except FileNotFoundError:
        logger.fatal("failed to load schema: file not found", file=str(schema_path))

    if event_file:
        try:
            with open(event_file, "r") as f:
                event = json.load(f)
        except json.JSONDecodeError as e:
            logger.fatal("failed to load event: invalid JSON", error=str(e), file=event_file)
        except FileNotFoundError:
            logger.fatal("failed to load event: file not found", file=event_file)
    else:
        try:
            event = json.load(sys.stdin)
        except json.JSONDecodeError as e:
            logger.fatal("failed to parse event from stdin", error=str(e))

    # Validate event against schema
    valid, error_msg = validate_event_against_schema(event, schema)
    if not valid:
        logger.fatal("event does not match schema", error=error_msg)

    event_timestamp = datetime.now(timezone.utc).isoformat()

    # If at_timestamp is provided, schedule the event for the future
    if at_timestamp:
        system_events_dir = get_event_stream_directory(data_dir, "system-events/scheduled")
        system_events_dir.mkdir(parents=True, exist_ok=True)

        (system_events_dir / "schemas").mkdir(exist_ok=True)
        (system_events_dir / "events").mkdir(exist_ok=True)

        scheduled_schema_path = system_events_dir / "schemas" / "schedule-task.schema"
        if not scheduled_schema_path.exists():
            # Get the schema from the embedded data section
            scheduled_schema = json.loads(get_embedded_data("system-events-scheduled-schema"))
            with open(scheduled_schema_path, "w") as f:
                json.dump(scheduled_schema, f, indent=2)

        scheduled_event = {
            "recorded_at": event_timestamp,
            "scheduled_for": at_timestamp,
            "event_stream": event_stream,
            "event_name": event_name,
            "event_payload": event,
            "process_immediately": process
        }

        # Acquire write lock for the scheduled events stream
        write_lock_path = system_events_dir / "write.flock"
        with CevLock(write_lock_path, "write"):
            current_index = get_current_stream_index(system_events_dir)
            new_index = increment_stream_index(current_index)

            event_path = get_event_path(
                system_events_dir,
                new_index,
                event_timestamp,
                "schedule-task"
            )

            with open(event_path, "w") as f:
                json.dump(scheduled_event, f, indent=2)

            set_current_stream_index(system_events_dir, new_index)

            logger.info("scheduled event",
                    event_stream=event_stream,
                    event_name=event_name,
                    scheduled_for=at_timestamp,
                    task_id=new_index)
        return

    # Use write lock for stream-specific event writing
    write_lock_path = event_stream_dir / "write.flock"
    with CevLock(write_lock_path, "write"):
        (event_stream_dir / "events").mkdir(parents=True, exist_ok=True)

        current_index = get_current_stream_index(event_stream_dir)
        new_index = increment_stream_index(current_index)

        event_path = get_event_path(
            event_stream_dir,
            new_index,
            event_timestamp,
            event_name
        )

        with open(event_path, "w") as f:
            json.dump(event, f, indent=2)

        set_current_stream_index(event_stream_dir, new_index)

        logger.info("added event",
                   event_stream=event_stream,
                   event_name=event_name,
                   stream_index=new_index)

    # Process event immediately if requested by the caller
    if process:
        process_args = argparse.Namespace()
        process_args.stream = event_stream
        process_args.trigger = None
        process_args.data_dir = args.data_dir
        process_args.verbose = args.verbose

        process_stream(process_args, logger, data_dir)

def define_trigger(args, logger, data_dir):
    event_stream = args.stream
    trigger_name = args.name
    trigger_path = args.trigger

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_name(trigger_name):
        logger.fatal("invalid trigger name", trigger_name=trigger_name)

    if not os.path.isfile(trigger_path):
        logger.fatal("trigger file does not exist", trigger_path=trigger_path)

    if not os.access(trigger_path, os.X_OK):
        logger.fatal("trigger file is not executable", trigger_path=trigger_path)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    trigger_dir = event_stream_dir / "triggers"
    events_dir = event_stream_dir / "events"

    trigger_dir.mkdir(parents=True, exist_ok=True)
    events_dir.mkdir(parents=True, exist_ok=True)

    dest_trigger_path = trigger_dir / trigger_name

    with open(trigger_path, "rb") as src, open(dest_trigger_path, "wb") as dest:
        dest.write(src.read())

    # Make sure it's executable
    dest_trigger_path.chmod(0o755)

    logger.info("defined trigger", event_stream=event_stream, trigger_name=trigger_name)

def alias_trigger(args, logger, data_dir):
    event_stream = args.stream
    trigger_name = args.name
    existing_trigger = args.trigger

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_name(trigger_name):
        logger.fatal("invalid trigger name", trigger_name=trigger_name)

    if not validate_event_name(existing_trigger):
        logger.fatal("invalid existing trigger name", existing_trigger=existing_trigger)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    trigger_dir = event_stream_dir / "triggers"

    if not trigger_dir.exists():
        logger.fatal("trigger directory does not exist for stream", event_stream=event_stream)

    existing_trigger_path = trigger_dir / existing_trigger

    if not existing_trigger_path.exists():
        logger.fatal("existing trigger does not exist", existing_trigger=existing_trigger)

    new_trigger_path = trigger_dir / trigger_name

    new_trigger_path.symlink_to(existing_trigger_path)

    logger.info("created trigger alias",
              event_stream=event_stream,
              trigger_name=trigger_name,
              existing_trigger=existing_trigger)

def process_stream(args, logger, data_dir):
    event_stream = args.stream
    specific_trigger = args.trigger

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    events_dir = event_stream_dir / "events"
    events_dir.mkdir(parents=True, exist_ok=True)

    logger.debug("process stream directories",
                data_dir=str(data_dir),
                event_stream_dir=str(event_stream_dir),
                events_dir=str(events_dir))

    state_lock_path = event_stream_dir / "state.flock"

    with CevLock(state_lock_path, "state"):
        trigger_dir = event_stream_dir / "triggers"
        state_dir = event_stream_dir / "state"
        dependency_dir = event_stream_dir / "dependencies"

        state_dir.mkdir(exist_ok=True)
        dependency_dir.mkdir(exist_ok=True)

        if not verify_dependency_state(event_stream_dir, data_dir, logger):
            add_stream_to_delayed(event_stream, event_stream_dir, logger)
            logger.info("stream processing delayed due to dependencies",
                      event_stream=event_stream)
            sys.exit(200)

        if specific_trigger:
            # Run specific trigger once
            trigger_path = trigger_dir / specific_trigger

            if not trigger_path.exists():
                logger.fatal("trigger does not exist", trigger=specific_trigger, event_stream=event_stream)

            # Get latest state snapshot
            latest_snapshot = get_latest_state_snapshot(event_stream_dir)

            env_vars = {
                "CEV_BIN": f"{sys.argv[0]} -d {data_dir}",
                "CEV_STREAM_STATE_DIR": str(state_dir),
                "CEV_STREAM_DEPENDENCY_DIR": str(dependency_dir),
                "CEV_TRIGGER_NAME": specific_trigger,
                "CEV_STREAM": event_stream,
                "CEV_LATEST_STATE_SNAPSHOT": str(latest_snapshot) if latest_snapshot else ""
            }

            # Log environment variables for debugging
            logger.debug("trigger environment variables", env_vars=str(env_vars))

            logger.info("running trigger", event_stream=event_stream, trigger_name=specific_trigger)

            # Track processing time for metadata
            start_time = time.time()
            exit_code, state_update = run_trigger(trigger_path, None, latest_snapshot, env_vars, logger, with_timeout=True)
            processing_duration_ms = int((time.time() - start_time) * 1000)

            # Handle special exit codes
            if exit_code == 200:
                # Delay execution by adding to delayed file
                logger.info("trigger requested delay due to dependency lock",
                           event_stream=event_stream,
                           trigger_name=specific_trigger)

                add_stream_to_delayed(event_stream, event_stream_dir, logger)
                sys.exit(0)
            elif exit_code == 201:
                # Do not retry
                logger.info("trigger requested no retry due to dependency lock",
                           event_stream=event_stream,
                           trigger_name=specific_trigger)
                sys.exit(0)
            elif exit_code == 202:
                # Dependency hasn't processed blocking event, schedule retry
                logger.info("trigger requested delay due to dependency not processing blocking event",
                           event_stream=event_stream,
                           trigger_name=specific_trigger)

                add_stream_to_delayed(event_stream, event_stream_dir, logger)
                sys.exit(0)
            elif exit_code != 0:
                logger.error("trigger failed with exit code",
                            exit_code=exit_code,
                            event_stream=event_stream,
                            trigger_name=specific_trigger)
                sys.exit(exit_code)

            # If we got state update, save it
            if state_update is not None:
                current_index = get_current_stream_index(event_stream_dir)
                processed_index = get_processed_stream_index(event_stream_dir)

                # Use the latest available index
                target_index = current_index if current_index else processed_index
                if not target_index:
                    target_index = "00000000"

                # Create state snapshot with processing duration
                snapshot_path = create_state_snapshot(
                    event_stream_dir,
                    data_dir,
                    event_stream,
                    target_index,
                    state_update,
                    processing_duration_ms
                )

                logger.info("created state snapshot from trigger output",
                           event_stream=event_stream,
                           trigger_name=specific_trigger,
                           snapshot=snapshot_path.name,
                           processing_duration_ms=processing_duration_ms)
        else:
            # Process all unprocessed events
            current_index = get_current_stream_index(event_stream_dir)
            processed_index = get_processed_stream_index(event_stream_dir)

            if current_index is None:
                logger.info("no events to process", event_stream=event_stream)
                return

            if processed_index is None:
                next_index = "00000000"
            else:
                next_index = increment_stream_index(processed_index)

            # Convert to integers for comparison
            current_index_int = int(current_index, 16)
            next_index_int = int(next_index, 16)

            final_state_update = None
            events_processed = 0
            blocked = False
            latest_processed_index = processed_index
            total_processing_time_ms = 0  # Track total processing time

            for idx in range(next_index_int, current_index_int + 1):
                if blocked:
                    break

                idx_hex = f"{idx:08x}"

                # Find the event file
                events_dir = event_stream_dir / "events"
                event_files = list(events_dir.glob(f"{idx_hex}-*"))

                if not event_files:
                    logger.error("event file not found for index",
                                stream_index=idx_hex,
                                event_stream=event_stream)
                    blocked = True
                    break

                event_path = event_files[0]

                # Extract event name from filename
                event_name = event_path.name.split("-", 2)[2].rsplit(".", 1)[0]

                # Find trigger for this event
                trigger_path = trigger_dir / event_name

                if not trigger_path.exists():
                    logger.error("trigger not found for event",
                                event_name=event_name,
                                event_stream=event_stream,
                                stream_index=idx_hex)
                    blocked = True
                    break

                # Extract timestamp from filename
                timestamp = event_path.name.split("-", 2)[1]

                # Get latest state snapshot
                latest_snapshot = get_latest_state_snapshot(event_stream_dir)

                env_vars = {
                    "CEV_BIN": f"{sys.argv[0]} -d {data_dir}",
                    "CEV_EVENT_ID": idx_hex,
                    "CEV_EVENT_TIMESTAMP": timestamp,
                    "CEV_STREAM_STATE_DIR": str(state_dir),
                    "CEV_STREAM_DEPENDENCY_DIR": str(dependency_dir),
                    "CEV_TRIGGER_NAME": event_name,
                    "CEV_STREAM": event_stream,
                    "CEV_LATEST_STATE_SNAPSHOT": str(latest_snapshot) if latest_snapshot else ""
                }

                logger.info("processing event",
                           event_stream=event_stream,
                           event_name=event_name,
                           stream_index=idx_hex)

                # Track processing time for metadata
                start_time = time.time()
                exit_code, state_update = run_trigger(trigger_path, event_path, latest_snapshot, env_vars, logger, with_timeout=True)
                event_processing_time_ms = int((time.time() - start_time) * 1000)
                total_processing_time_ms += event_processing_time_ms

                # Handle special exit codes
                if exit_code == 200:
                    logger.info("trigger requested delay due to dependency lock",
                               event_stream=event_stream,
                               event_name=event_name,
                               stream_index=idx_hex)

                    add_stream_to_delayed(event_stream, event_stream_dir, logger)
                    blocked = True
                    break
                elif exit_code == 201:
                    logger.info("trigger requested no retry due to dependency lock",
                               event_stream=event_stream,
                               event_name=event_name,
                               stream_index=idx_hex)
                    blocked = True
                    break
                elif exit_code == 202:
                    logger.info("trigger requested delay due to dependency not processing blocking event",
                               event_stream=event_stream,
                               event_name=event_name,
                               stream_index=idx_hex)

                    add_stream_to_delayed(event_stream, event_stream_dir, logger)
                    blocked = True
                    break
                elif exit_code != 0:
                    logger.error("trigger failed with exit code",
                                exit_code=exit_code,
                                event_stream=event_stream,
                                event_name=event_name,
                                stream_index=idx_hex)
                    blocked = True
                    break

                # Update processed index
                set_processed_stream_index(event_stream_dir, idx_hex)
                latest_processed_index = idx_hex
                events_processed += 1

                # Update final state if we got an update
                if state_update is not None:
                    final_state_update = state_update

            # Create state snapshot if we have processed events and received state updates
            if events_processed > 0 and final_state_update is not None:
                snapshot_path = create_state_snapshot(
                    event_stream_dir,
                    data_dir,
                    event_stream,
                    latest_processed_index,
                    final_state_update,
                    total_processing_time_ms
                )

                logger.info("created state snapshot for processed events",
                           event_stream=event_stream,
                           event_count=events_processed,
                           snapshot=snapshot_path.name,
                           processing_duration_ms=total_processing_time_ms)

            logger.info("processed events",
                       event_stream=event_stream,
                       count=events_processed,
                       blocked=blocked)

            # Check delayed file and process
            delayed_path = event_stream_dir / "delayed"
            delay_lock_path = event_stream_dir / "delayed.flock"

            if delayed_path.exists():
                with CevLock(delay_lock_path, "delayed"):
                    try:
                        delayed_streams = set()
                        with open(delayed_path, "r") as f:
                            for line in f:
                                delayed_streams.add(line.strip())

                        # Remove this stream from delayed if present
                        if event_stream in delayed_streams:
                            delayed_streams.remove(event_stream)

                            # Rewrite the delayed file
                            with open(delayed_path, "w") as f:
                                for stream in delayed_streams:
                                    f.write(f"{stream}\n")

                            logger.info("removed stream from delayed file",
                                    event_stream=event_stream)
                    except Exception as e:
                        logger.error("failed to process delayed file",
                                    error=str(e),
                                    event_stream=event_stream)

def depends_on(args, logger, data_dir):
    event_stream = args.stream
    target_stream = args.target

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    if not validate_event_stream(target_stream):
        logger.fatal("invalid target stream name", target_stream=target_stream)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)
    target_stream_dir = get_event_stream_directory(data_dir, target_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    if not target_stream_dir.exists():
        logger.fatal("target stream does not exist", target_stream=target_stream)

    dependency_dir = event_stream_dir / "dependencies"
    dependency_dir.mkdir(exist_ok=True)

    # Create symlink to target stream
    dependency_link = dependency_dir / target_stream

    # Remove existing link if it exists
    if dependency_link.exists():
        dependency_link.unlink()

    dependency_link.symlink_to(target_stream_dir)

    logger.info("added dependency",
               event_stream=event_stream,
               target_stream=target_stream)

def is_dirty(args, logger, data_dir):
    event_stream = args.stream

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    current_index = get_current_stream_index(event_stream_dir)
    processed_index = get_processed_stream_index(event_stream_dir)

    if current_index is None:
        # No events
        logger.info("stream is clean (no events)", event_stream=event_stream, status="clean")
        sys.exit(0)

    if processed_index is None:
        logger.info("stream is dirty (no processed events)", event_stream=event_stream, status="dirty")
        sys.exit(1)

    # Check if current is ahead of processed
    if int(current_index, 16) > int(processed_index, 16):
        logger.info("stream is dirty (unprocessed events)",
                   event_stream=event_stream,
                   current=current_index,
                   processed=processed_index,
                   status="dirty")
        sys.exit(1)

    # Check dependencies
    dependency_dir = event_stream_dir / "dependencies"
    if dependency_dir.exists():
        for dependency_link in dependency_dir.glob("*"):
            if not dependency_link.is_symlink():
                continue

            dependency_stream = dependency_link.name
            dependency_dir_path = data_dir / dependency_stream

            if not dependency_dir_path.exists():
                logger.error("dependency target does not exist",
                            dependency_stream=dependency_stream,
                            event_stream=event_stream)
                sys.exit(1)

            # Get dependency state information
            dependency_processed = get_processed_stream_index(dependency_dir_path)
            dependency_current = get_current_stream_index(dependency_dir_path)

            if dependency_processed is None:
                dependency_processed = dependency_current

            if dependency_processed is None:
                # No events to process
                continue

            # Check if we have incorporated the latest processed events from dependency
            latest_snapshot = get_latest_state_snapshot(event_stream_dir)

            if latest_snapshot is None:
                # No state snapshot yet, need to process
                logger.info("stream is dirty (no state snapshot)",
                            event_stream=event_stream,
                            dependency=dependency_stream,
                            status="dirty")
                sys.exit(1)

            # Get recorded dependency versions
            metadata, _ = parse_state_snapshot(latest_snapshot)
            dependencies = metadata.get("dependencies", {})

            if dependency_stream not in dependencies:
                # Dependency not recorded, need to process
                logger.info("stream is dirty (dependency not recorded)",
                            event_stream=event_stream,
                            dependency=dependency_stream,
                            status="dirty")
                sys.exit(1)

            dep_info = dependencies[dependency_stream]
            recorded_event_id = dep_info.get("event_id", "00000000")

            if int(recorded_event_id, 16) < int(dependency_processed, 16):
                logger.info("stream is dirty (dependency ahead)",
                            event_stream=event_stream,
                            dependency=dependency_stream,
                            current=recorded_event_id,
                            dependency_processed=dependency_processed,
                            status="dirty")
                sys.exit(1)

    logger.info("stream is clean", event_stream=event_stream, status="clean")
    sys.exit(0)

def get_state(args, logger, data_dir):
    event_stream = args.stream
    raw_flag = args.raw

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    # Get latest state snapshot
    latest_snapshot = get_latest_state_snapshot(event_stream_dir)

    if latest_snapshot is None:
        if raw_flag:
            # Return empty object for raw state
            sys.stdout.write("{}\n")
        else:
            # Return empty state with metadata
            empty_state = {
                "metadata": {
                    "stream": event_stream,
                    "previous_snapshot": "",
                    "event_id": "",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "dependencies": {}
                },
                "state": {}
            }
            sys.stdout.write(json.dumps(empty_state, indent=2) + "\n")
        return

    # Parse the state snapshot
    try:
        with open(latest_snapshot, "r") as f:
            snapshot = json.load(f)

        if raw_flag:
            # Return only the raw state
            state = snapshot.get("state", {})
            sys.stdout.write(json.dumps(state, indent=2) + "\n")
        else:
            # Return the full snapshot with metadata
            sys.stdout.write(json.dumps(snapshot, indent=2) + "\n")

    except (json.JSONDecodeError, FileNotFoundError) as e:
        logger.error("failed to read state snapshot",
                   snapshot=str(latest_snapshot),
                   error=str(e))
        sys.exit(1)

def list_snapshots(args, logger, data_dir):
    event_stream = args.stream
    event_count = args.n
    all_snapshots = args.a

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    state_dir = event_stream_dir / "state"
    if not state_dir.exists():
        # No snapshots yet
        snapshots = []
    else:
        snapshots = sorted(state_dir.glob("*.state.json"), key=lambda x: str(x))

        # Apply limit if not requesting all snapshots
        if not all_snapshots and event_count is not None:
            snapshots = snapshots[-event_count:]

    # Create output with snapshot info
    output = {
        "stream": event_stream,
        "snapshots": []
    }

    for snapshot in snapshots:
        try:
            metadata, _ = parse_state_snapshot(snapshot)
            snapshot_info = {
                "file": snapshot.name,
                "event_id": metadata.get("event_id", ""),
                "timestamp": metadata.get("timestamp", ""),
                "dependencies": metadata.get("dependencies", {})
            }

            # Add processing_duration_ms if available
            if "processing_duration_ms" in metadata:
                snapshot_info["processing_duration_ms"] = metadata["processing_duration_ms"]

            output["snapshots"].append(snapshot_info)
        except Exception as e:
            logger.warn("failed to parse snapshot metadata",
                      snapshot=snapshot.name,
                      error=str(e))
            output["snapshots"].append({
                "file": snapshot.name,
                "error": "failed to parse metadata"
            })

    # Write the output
    sys.stdout.write(json.dumps(output, indent=2) + "\n")

def is_newer(args, logger, data_dir):
    event_stream = args.stream
    stream_index = args.i

    if not validate_event_stream(event_stream):
        logger.fatal("invalid event stream name", event_stream=event_stream)

    # Validate stream index format
    try:
        int(stream_index, 16)
    except ValueError:
        logger.fatal("invalid stream index format", stream_index=stream_index)

    event_stream_dir = get_event_stream_directory(data_dir, event_stream)

    if not event_stream_dir.exists():
        logger.fatal("event stream does not exist", event_stream=event_stream)

    # Get latest state snapshot
    latest_snapshot = get_latest_state_snapshot(event_stream_dir)

    if latest_snapshot is None:
        logger.info("stream has no snapshots", event_stream=event_stream)
        sys.exit(1)

    # Get metadata from snapshot
    metadata, _ = parse_state_snapshot(latest_snapshot)
    snapshot_event_id = metadata.get("event_id", "")

    if not snapshot_event_id:
        logger.info("snapshot has no event_id", event_stream=event_stream)
        sys.exit(1)

    # Compare event IDs
    if int(snapshot_event_id, 16) > int(stream_index, 16):
        logger.info("snapshot is newer than provided index",
                   event_stream=event_stream,
                   snapshot_id=snapshot_event_id,
                   provided_index=stream_index)
        sys.exit(0)
    else:
        logger.info("snapshot is not newer than provided index",
                   event_stream=event_stream,
                   snapshot_id=snapshot_event_id,
                   provided_index=stream_index)
        sys.exit(1)

def system_info(args, logger, data_dir):
    stream_count = 0
    stream_info = []

    for item in data_dir.glob("*"):
        if item.is_dir() and item.name != "system-events":
            stream_info.append(get_stream_info(item))
            stream_count += 1

    # Count system-events separately
    system_events_dir = data_dir / "system-events"
    if system_events_dir.exists():
        for item in system_events_dir.glob("*"):
            if item.is_dir():
                stream_info.append(get_stream_info(system_events_dir / item.name))
                stream_count += 1

    info = {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "level": "info",
        "message": "system info",
        "data_directory": str(data_dir),
        "cev_binary": sys.argv[0],
        "version": VERSION,
        "stream_count": stream_count,
        "streams": stream_info
    }

    # Output structured JSON
    sys.stdout.write(json.dumps(info, indent=2) + "\n")

    logger.debug("displayed system info")

def get_stream_info(stream_dir: Path) -> Dict[str, Any]:
    """Get detailed information about a stream"""
    stream_name = str(stream_dir.relative_to(stream_dir.parent))

    current_idx = get_current_stream_index(stream_dir)
    processed_idx = get_processed_stream_index(stream_dir)

    # Count events and triggers
    events_dir = stream_dir / "events"
    triggers_dir = stream_dir / "triggers"

    event_count = len(list(events_dir.glob("*.evt"))) if events_dir.exists() else 0
    trigger_count = len(list(triggers_dir.glob("*"))) if triggers_dir.exists() else 0

    # Get event types
    schemas_dir = stream_dir / "schemas"
    event_types = []

    if schemas_dir.exists():
        for schema_file in schemas_dir.glob("*.schema"):
            event_type = schema_file.name.replace(".schema", "")
            has_trigger = (triggers_dir / event_type).exists() if triggers_dir.exists() else False
            event_types.append({
                "name": event_type,
                "has_trigger": has_trigger
            })

    # Get snapshot count
    state_dir = stream_dir / "state"
    snapshot_count = len(list(state_dir.glob("*.state.json"))) if state_dir.exists() else 0

    return {
        "name": stream_name,
        "current_index": current_idx,
        "processed_index": processed_idx,
        "event_count": event_count,
        "trigger_count": trigger_count,
        "event_types": event_types,
        "snapshot_count": snapshot_count
    }

def init_system_events(data_dir: Path, logger: CevLogger) -> bool:
    """Initialize system events directories and schemas if they don't exist.
    Returns True if initialized, False if already set up."""

    # Check if already initialized to avoid unnecessary work
    scheduled_dir = get_event_stream_directory(data_dir, "system-events/scheduled")
    if scheduled_dir.exists() and (scheduled_dir / "schemas" / "schedule-task.schema").exists():
        return False

    # Create system-events/scheduled stream and schema
    scheduled_dir.mkdir(parents=True, exist_ok=True)

    schemas_dir = scheduled_dir / "schemas"
    triggers_dir = scheduled_dir / "triggers"
    events_dir = scheduled_dir / "events"
    state_dir = scheduled_dir / "state"

    # Create directories
    schemas_dir.mkdir(exist_ok=True)
    triggers_dir.mkdir(exist_ok=True)
    events_dir.mkdir(exist_ok=True)
    state_dir.mkdir(exist_ok=True)

    # Create schemas from embedded data
    schedule_task_schema = get_embedded_data("system-events-scheduled-schema")
    cancel_task_schema = get_embedded_data("system-events-cancel-schema")
    complete_task_schema = get_embedded_data("system-events-complete-schema")

    # Write schemas
    schedule_task_schema_path = schemas_dir / "schedule-task.schema"
    if not schedule_task_schema_path.exists():
        with open(schedule_task_schema_path, "w") as f:
            json.dump(schedule_task_schema, f, indent=2)

    cancel_task_schema_path = schemas_dir / "cancel-task.schema"
    if not cancel_task_schema_path.exists():
        with open(cancel_task_schema_path, "w") as f:
            json.dump(cancel_task_schema, f, indent=2)

    complete_task_schema_path = schemas_dir / "complete-task.schema"
    if not complete_task_schema_path.exists():
        with open(complete_task_schema_path, "w") as f:
            json.dump(complete_task_schema, f, indent=2)

    # Create cron trigger
    cron_trigger_path = triggers_dir / "cron"

    if not cron_trigger_path.exists():
        cron_trigger_content = get_embedded_data("cron-trigger")
        with open(cron_trigger_path, "w") as f:
            f.write(cron_trigger_content)

        cron_trigger_path.chmod(0o755)

    logger.debug("initialized system events")
    return True

def main():
    parser = argparse.ArgumentParser(description="Common EVent system (cev)")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
    parser.add_argument("-d", "--data-dir", help="Override default data directory")
    parser.add_argument("--version", action="version", version=f"cev {VERSION}")

    subparsers = parser.add_subparsers(dest="command", required=True)

    # define-event
    define_event_parser = subparsers.add_parser("define-event", help="Define an event schema")
    define_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    define_event_parser.add_argument("-n", "--name", required=True, help="Event name")
    define_event_parser.add_argument("-f", "--file", help="JSON schema file path")

    # create-event
    create_event_parser = subparsers.add_parser("create-event", help="Create an event stub")
    create_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    create_event_parser.add_argument("-n", "--name", required=True, help="Event name")
    create_event_parser.add_argument("-f", "--file", help="Output file path")

    # add-event
    add_event_parser = subparsers.add_parser("add-event", help="Add an event to a stream")
    add_event_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    add_event_parser.add_argument("-n", "--name", required=True, help="Event name")
    add_event_parser.add_argument("-f", "--file", help="Event JSON file path")
    add_event_parser.add_argument("-a", "--at", help="Schedule event for a future time (ISO-8601)")
    add_event_parser.add_argument("-p", "--process", action="store_true", help="Process stream after adding event")

    # define-trigger
    define_trigger_parser = subparsers.add_parser("define-trigger", help="Define a trigger")
    define_trigger_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    define_trigger_parser.add_argument("-n", "--name", required=True, help="Trigger name")
    define_trigger_parser.add_argument("-t", "--trigger", required=True, help="Trigger executable path")

    # alias-trigger
    alias_trigger_parser = subparsers.add_parser("alias-trigger", help="Create a trigger alias")
    alias_trigger_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    alias_trigger_parser.add_argument("-n", "--name", required=True, help="New trigger name")
    alias_trigger_parser.add_argument("-t", "--trigger", required=True, help="Existing trigger name")

    # process
    process_parser = subparsers.add_parser("process", help="Process events in a stream")
    process_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    process_parser.add_argument("-t", "--trigger", help="Specific trigger to run")

    # depends-on
    depends_on_parser = subparsers.add_parser("depends-on", help="Add a dependency to a stream")
    depends_on_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    depends_on_parser.add_argument("-t", "--target", required=True, help="Target stream name")

    # get-state
    get_state_parser = subparsers.add_parser("get-state", help="Get the current state of a stream")
    get_state_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    get_state_parser.add_argument("-r", "--raw", action="store_true", help="Return only the raw state without metadata envelope")

    # list-snapshots
    list_snapshots_parser = subparsers.add_parser("list-snapshots", help="List all state snapshots for a stream")
    list_snapshots_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    list_snapshots_parser.add_argument("-n", type=int, default=10, help="Return the latest N snapshots (default 10)")
    list_snapshots_parser.add_argument("-a", action="store_true", help="Return all snapshots (overrides -n)")

    # is-dirty
    is_dirty_parser = subparsers.add_parser("is-dirty", help="Check if a stream is dirty")
    is_dirty_parser.add_argument("-s", "--stream", required=True, help="Event stream name")

    # is-newer
    is_newer_parser = subparsers.add_parser("is-newer", help="Check if a stream's latest state snapshot is newer than provided index")
    is_newer_parser.add_argument("-s", "--stream", required=True, help="Event stream name")
    is_newer_parser.add_argument("-i", required=True, help="Stream index to compare against")

    # info
    subparsers.add_parser("info", help="Display system information")

    args = parser.parse_args()

    # Set up data directory and logger
    data_dir = get_data_directory(args)
    data_dir.mkdir(parents=True, exist_ok=True)

    logger = CevLogger(data_dir / "system.log", args.verbose)

    # Initialize system events
    init_system_events(data_dir, logger)

    # Route to the appropriate command
    if args.command == "define-event":
        define_event(args, logger, data_dir)
    elif args.command == "create-event":
        create_event(args, logger, data_dir)
    elif args.command == "add-event":
        add_event(args, logger, data_dir)
    elif args.command == "define-trigger":
        define_trigger(args, logger, data_dir)
    elif args.command == "alias-trigger":
        alias_trigger(args, logger, data_dir)
    elif args.command == "process":
        process_stream(args, logger, data_dir)
    elif args.command == "depends-on":
        depends_on(args, logger, data_dir)
    elif args.command == "get-state":
        get_state(args, logger, data_dir)
    elif args.command == "list-snapshots":
        list_snapshots(args, logger, data_dir)
    elif args.command == "is-dirty":
        is_dirty(args, logger, data_dir)
    elif args.command == "is-newer":
        is_newer(args, logger, data_dir)
    elif args.command == "info":
        system_info(args, logger, data_dir)

if __name__ == "__main__":
    main()

# Stop processing before reaching the data section
sys.exit(0)

# BEGIN_SCRIPTS
# [system-events-scheduled-schema]
{
  "type": "object",
  "required": ["recorded_at", "scheduled_for", "event_stream"],
  "properties": {
    "recorded_at": {"type": "string", "format": "date-time"},
    "scheduled_for": {"type": "string", "format": "date-time"},
    "event_stream": {"type": "string"},
    "event_name": {"type": "string"},
    "event_payload": {"type": "object"},
    "process_immediately": {"type": "boolean"}
  }
}

# [system-events-cancel-schema]
{
  "type": "object",
  "required": ["task_id"],
  "properties": {
    "task_id": {"type": "string"},
    "reason": {"type": "string"}
  }
}

# [system-events-complete-schema]
{
  "type": "object",
  "required": ["task_id"],
  "properties": {
    "task_id": {"type": "string"}
  }
}

# [cron-trigger]
#!/usr/bin/env python3
import json
import os
import signal
import subprocess
import sys
import time
from datetime import datetime, timezone
from pathlib import Path

# Constants from specification
MAX_RUNTIME_SECONDS = 4 * 60 + 30  # 4 minutes and 30 seconds
TRIGGER_TIMEOUT_SECONDS = 60  # 1 minute
TRIGGER_KILL_GRACE_PERIOD = 15  # 15 seconds

def log_msg(level, message, **kwargs):
    log_entry = {"level": level, "message": message, **kwargs}
    print(json.dumps(log_entry), file=sys.stderr)

def read_state(state_dir):
    state_file = Path(state_dir) / "scheduled_tasks.json"
    if not state_file.exists():
        return []

    try:
        with open(state_file, "r") as f:
            return json.load(f)
    except json.JSONDecodeError:
        log_msg("error", "failed to parse state file")
        return []

def write_state(state_dir, tasks):
    state_file = Path(state_dir) / "scheduled_tasks.json"
    with open(state_file, "w") as f:
        json.dump(tasks, f, indent=2)

def parse_event(event_path):
    try:
        with open(event_path, "r") as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        log_msg("error", f"failed to parse event: {event_path}")
        return None

def execute_trigger_with_timeout(cev_bin, event_stream, trigger_name=None):
    # Execute a trigger with proper timeout handling
    cmd = [cev_bin, "process", "-s", event_stream]
    if trigger_name:
        cmd.extend(["-t", trigger_name])

    log_msg("info", "executing trigger", cmd=str(cmd))

    try:
        process = subprocess.Popen(cmd)

        # Wait for a maximum of TRIGGER_TIMEOUT_SECONDS
        start_time = time.time()
        while process.poll() is None:
            if time.time() - start_time > TRIGGER_TIMEOUT_SECONDS:
                # Send SIGHUP first
                log_msg("warn", "trigger exceeded timeout, sending SIGHUP", trigger=trigger_name or event_stream)
                process.send_signal(signal.SIGHUP)

                # Wait additional grace period before SIGKILL
                grace_time = time.time()
                while process.poll() is None and time.time() - grace_time < TRIGGER_KILL_GRACE_PERIOD:
                    time.sleep(0.1)

                # If still running, SIGKILL
                if process.poll() is None:
                    log_msg("warn", "trigger still running after SIGHUP, sending SIGKILL")
                    process.kill()
                    break

            # Small sleep to avoid spinning
            time.sleep(0.1)

        return process.returncode
    except Exception as e:
        log_msg("error", "error executing trigger", error=str(e))
        return 1

def main():
    start_time = time.time()

    # Environment variables
    cev_bin = os.environ.get("CEV_BIN")
    state_dir = os.environ.get("CEV_STREAM_STATE_DIR")

    if not cev_bin or not state_dir:
        log_msg("error", "missing required environment variables")
        sys.exit(1)

    # Process any unprocessed events first
    subprocess.run([cev_bin, "process", "-s", "system-events/scheduled"])

    # Read the current state
    tasks = read_state(state_dir)

    # Find all event files
    event_files = []
    event_stream_dir = Path(state_dir).parent
    events_dir = event_stream_dir / "events"

    # Process scheduled tasks
    for event_file in events_dir.glob("*-schedule-task.evt"):
        event_id = event_file.name.split("-")[0]
        event = parse_event(event_file)

        if event:
            # Add to task list if not already there
            if not any(task.get("task_id") == event_id for task in tasks):
                tasks.append({
                    "task_id": event_id,
                    "scheduled_for": event["scheduled_for"],
                    "event": event
                })

    # Process canceled tasks
    cancel_events = []
    for event_file in events_dir.glob("*-cancel-task.evt"):
        event = parse_event(event_file)
        if event and "task_id" in event:
            cancel_events.append(event["task_id"])
            log_msg("info", "task canceled", task_id=event["task_id"], reason=event.get("reason", "no reason provided"))

    # Process completed tasks
    complete_events = []
    for event_file in events_dir.glob("*-complete-task.evt"):
        event = parse_event(event_file)
        if event and "task_id" in event:
            complete_events.append(event["task_id"])

    # Remove canceled and completed tasks
    tasks = [task for task in tasks
             if task.get("task_id") not in cancel_events
             and task.get("task_id") not in complete_events]

    # Sort tasks by scheduled_for
    tasks.sort(key=lambda x: x.get("scheduled_for", ""))

    # Save updated state
    write_state(state_dir, tasks)

    # Execute due tasks
    now = datetime.now(timezone.utc).isoformat()
    due_tasks = [task for task in tasks if task.get("scheduled_for", "") <= now]

    executed_tasks = []

    for task in due_tasks:
        task_id = task.get("task_id")

        # Get full event data from saved state
        event = task.get("event", {})
        if not event:
            log_msg("error", "missing event data for task", task_id=task_id)
            executed_tasks.append(task_id)  # Mark as executed to remove from list
            continue

        # Check if we have at least 1.5 minutes left in our window
        time_elapsed = time.time() - start_time
        time_remaining = MAX_RUNTIME_SECONDS - time_elapsed

        if time_remaining < 90:
            log_msg("info", "insufficient time remaining, skipping remaining tasks")
            break

        log_msg("info", "executing scheduled task", task_id=task_id)

        # Get task details
        event_payload = event.get("event_payload", {})
        event_stream = event.get("event_stream")
        event_name = event.get("event_name")
        process_immediately = event.get("process_immediately", False)
        recorded_at = event.get("recorded_at")

        if not event_stream:
            log_msg("error", "missing event stream in scheduled task")
            executed_tasks.append(task_id)
            continue

        # If event_name is not provided, just process the stream
        if not event_name:
            execute_trigger_with_timeout(cev_bin, event_stream)
            executed_tasks.append(task_id)
            continue

        # Create a temporary file for the event payload
        temp_dir = Path("/tmp")
        temp_file = temp_dir / f"cev-scheduled-{task_id}.json"

        with open(temp_file, "w") as f:
            json.dump(event_payload, f)

        # Add the event to the stream, using the original recorded_at timestamp
        cmd = [cev_bin, "add-event", "-s", event_stream, "-n", event_name, "-f", str(temp_file)]

        if process_immediately:
            cmd.append("-p")

        subprocess.run(cmd)

        # Clean up the temporary file
        temp_file.unlink()

        # Add a complete-task event
        complete_payload = {"task_id": task_id}
        complete_temp_file = temp_dir / f"cev-complete-{task_id}.json"

        with open(complete_temp_file, "w") as f:
            json.dump(complete_payload, f)

        subprocess.run([
            cev_bin, "add-event", "-s", "system-events/scheduled",
            "-n", "complete-task", "-f", str(complete_temp_file)
        ])

        complete_temp_file.unlink()

        executed_tasks.append(task_id)

    # Remove executed tasks from state
    tasks = [task for task in tasks if task.get("task_id") not in executed_tasks]

    # Save final state
    write_state(state_dir, tasks)

    # If any tasks were executed, log success
    if executed_tasks:
        log_msg("info", "completed scheduled tasks", executed_count=len(executed_tasks))

    return 0

if __name__ == "__main__":
    sys.exit(main())
